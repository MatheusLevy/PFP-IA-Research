{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Managing model stages: YOLO_Model_v2 v1\n",
      "‚úÖ Model promoted to PRODUCTION\n",
      "‚úÖ Description updated\n",
      "‚úÖ Tags added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14654/926385053.py:19: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9444'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAIOSFODNN7EXAMPLE'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "def manage_model_stages(model_name, version=\"1\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(f\"üîÑ Managing model stages: {model_name} v{version}\")\n",
    "    \n",
    "    try:\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=\"Production\",\n",
    "            archive_existing_versions=False\n",
    "        )\n",
    "        print(\"‚úÖ Model promoted to PRODUCTION\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error promoting to production: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            description=\"Model under test - Performance validated on validation dataset\"\n",
    "        )\n",
    "        print(\"‚úÖ Description updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating description: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"performance_tier\",\n",
    "            value=\"high\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Tags added\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding tags: {e}\")\n",
    "\n",
    "manage_model_stages(\"YOLO_Model_v2\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b26f49",
   "metadata": {},
   "source": [
    "# Train new model optmizition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29084c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "models = [\"yolo12n\", \"yolo12s\"]\n",
    "data = \"./dataset/data_freeze.yaml\"\n",
    "epochs= 3\n",
    "patience= 100\n",
    "min_batch_size= 8\n",
    "max_batch_size= 8\n",
    "batch_size_step = 4\n",
    "image_size = [256, 512]\n",
    "cache = False\n",
    "optimizer = [\"SGD\", \"Adam\", \"AdamW\", \"NAdam\", \"RAdam\", \"RMSProp\"]\n",
    "multi_scale = [True, False]\n",
    "cos_lr = [True, False]\n",
    "# close_mosaic_min = 1\n",
    "# close_mosaic_max = 100\n",
    "amp = True\n",
    "lr0_min = 1e-5\n",
    "lr0_max = 1e-1\n",
    "# momentum_min = 0.5\n",
    "# momentum_max = 0.99\n",
    "# weight_decay_min = 1e-6\n",
    "# weight_decay_max = 1e-2\n",
    "# warmup_epochs_min = 1\n",
    "# warmup_epochs_max = 100\n",
    "# warmup_momentum_min = 0.5\n",
    "# warmup_momentum_max = 0.99\n",
    "# warmup_bias_lr_min = 1e-5\n",
    "# warmup_bias_lr_max = 0.2\n",
    "# box_weight_min = 1e-5\n",
    "# box_weight_max = 10.0\n",
    "# cls_weight_min = 1e-5\n",
    "# cls_weight_max = 10.0\n",
    "# dfl_weight_min = 1e-5\n",
    "# dfl_weight_max = 10.0\n",
    "dropout_min = 1e-5\n",
    "dropout_max = 0.5\n",
    "max_workers = max(1, (len(os.sched_getaffinity(0)) if hasattr(os, 'sched_getaffinity') else os.cpu_count()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411336be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_device():\n",
    "  device = \"cpu\"\n",
    "  if (torch.cuda.is_available()):\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    device = [-1] * num_gpus\n",
    "  return device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df07006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import datetime as dt\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "\n",
    "def train(trial):\n",
    "    today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    group_name = f\"study-{today}__build_{os.getenv('GIT_SHA','local')}__data_v42\"\n",
    "    run_name = f\"trial/{trial.number}\"\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"yolo-optimization-monitor\",\n",
    "        entity=\"levybessa-puc\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "        tags=[\"YOLO\", \"optuna\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    wandb.log({\n",
    "        \"status\": \"running\",\n",
    "        \"trial_number\": trial.number,\n",
    "        \"stage\": \"initializing\",\n",
    "    })\n",
    "    \n",
    "    print(f\"üöÄ Started wandb run: {run_name} (ID: {run.id})\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Model selection\n",
    "        model_name = trial.suggest_categorical(\"model\", models)\n",
    "        \n",
    "        # Basic training parameters\n",
    "        batch_size = trial.suggest_int(\"batch_size\", min_batch_size, max_batch_size, step=batch_size_step)\n",
    "        imgsz = trial.suggest_categorical(\"imgsz\", image_size)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", optimizer)\n",
    "        \n",
    "        # Training settings\n",
    "        multi_scale_enabled = trial.suggest_categorical(\"multi_scale\", multi_scale)\n",
    "        # cos_lr_enabled = trial.suggest_categorical(\"cos_lr\", cos_lr)\n",
    "        # close_mosaic = trial.suggest_int(\"close_mosaic\", close_mosaic_min, close_mosaic_max)\n",
    "        \n",
    "        # Learning rate parameters\n",
    "        lr0 = trial.suggest_float(\"lr0\", lr0_min, lr0_max, log=True)\n",
    "        # momentum = trial.suggest_float(\"momentum\", momentum_min, momentum_max)\n",
    "        # weight_decay = trial.suggest_float(\"weight_decay\", weight_decay_min, weight_decay_max, log=True)\n",
    "        \n",
    "        # Warm-up parameters\n",
    "        # warmup_epochs = trial.suggest_int(\"warmup_epochs\", warmup_epochs_min, warmup_epochs_max)\n",
    "        # warmup_momentum = trial.suggest_float(\"warmup_momentum\", warmup_momentum_min, warmup_momentum_max)\n",
    "        # warmup_bias_lr = trial.suggest_float(\"warmup_bias_lr\", warmup_bias_lr_min, warmup_bias_lr_max)\n",
    "        \n",
    "        # Loss function weights\n",
    "        # box_weight = trial.suggest_float(\"box_weight\", box_weight_min, box_weight_max)\n",
    "        # cls_weight = trial.suggest_float(\"cls_weight\", cls_weight_min, cls_weight_max)\n",
    "        # dfl_weight = trial.suggest_float(\"dfl_weight\", dfl_weight_min, dfl_weight_max)\n",
    "        \n",
    "        # Dropout\n",
    "        dropout = trial.suggest_float(\"dropout\", dropout_min, dropout_max)\n",
    "        \n",
    "        hyperparams = {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"imgsz\": imgsz,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"multi_scale\": multi_scale_enabled,\n",
    "            # \"cos_lr\": cos_lr_enabled,\n",
    "            # \"close_mosaic\": close_mosaic,\n",
    "            \"lr0\": lr0,\n",
    "            # \"momentum\": momentum,\n",
    "            # \"weight_decay\": weight_decay,\n",
    "            # \"warmup_epochs\": warmup_epochs,\n",
    "            # \"warmup_momentum\": warmup_momentum,\n",
    "            # \"warmup_bias_lr\": warmup_bias_lr,\n",
    "            # \"box_weight\": box_weight,\n",
    "            # \"cls_weight\": cls_weight,\n",
    "            # \"dfl_weight\": dfl_weight,\n",
    "            \"dropout\": dropout,\n",
    "            # Configura√ß√µes fixas\n",
    "            \"epochs\": epochs,\n",
    "            \"patience\": patience,\n",
    "            \"cache\": cache,\n",
    "            \"amp\": amp,\n",
    "            \"data\": data,\n",
    "            \"max_workers\": max_workers\n",
    "        }\n",
    "        \n",
    "        wandb.config.update(hyperparams)\n",
    "        \n",
    "        print(f\"üìã Trial {trial.number} parameters:\")\n",
    "        print(f\"   Model: {model_name}, Batch: {batch_size}, ImgSz: {imgsz}\")\n",
    "        print(f\"   Optimizer: {optimizer_name}, LR: {lr0:.6f}\")\n",
    "        print(f\"   Group: {group_name}\")\n",
    "        print(f\"   Run: {run_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        \n",
    "        model = YOLO(f\"{model_name}.pt\")\n",
    "        \n",
    "\n",
    "        print(f\"üèãÔ∏è Starting training for trial {trial.number}...\")\n",
    "        \n",
    "        results = model.train(\n",
    "            project=None, \n",
    "            data=data,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            cache=cache,\n",
    "            optimizer=optimizer_name,\n",
    "            multi_scale=multi_scale_enabled,\n",
    "            # cos_lr=cos_lr_enabled,\n",
    "            # close_mosaic=close_mosaic,\n",
    "            amp=amp,\n",
    "            lr0=lr0,\n",
    "            # momentum=momentum,\n",
    "            # weight_decay=weight_decay,\n",
    "            # warmup_epochs=warmup_epochs,\n",
    "            # warmup_momentum=warmup_momentum,\n",
    "            # warmup_bias_lr=warmup_bias_lr,\n",
    "            # box=box_weight,\n",
    "            # cls=cls_weight,\n",
    "            # dfl=dfl_weight,\n",
    "            dropout=dropout,\n",
    "            verbose=False,\n",
    "            save=False,\n",
    "            plots=True,\n",
    "            device=get_device(),\n",
    "            workers=max_workers\n",
    "        )\n",
    "        \n",
    "        mean_precision, mean_recall, mAP50, mAP50_90 = tuple(results.box.mean_results())\n",
    "        \n",
    "        final_metrics = {\n",
    "            \"mAP50_95\": mAP50_90,\n",
    "            \"mean precision\": mean_precision,\n",
    "            \"mean recall\": mean_recall,\n",
    "            \"mAP50\": mAP50,\n",
    "            \"status\": \"completed\",\n",
    "            \"stage\": \"finished\",\n",
    "            \"trial_number\": trial.number,\n",
    "        }\n",
    "        \n",
    "            \n",
    "        wandb.log(final_metrics)\n",
    "        wandb.finish()\n",
    "        print(f\"‚úÖ Trial {trial.number} completed successfully: mAP = {mAP50_90:.4f}\")\n",
    "        print(f\"üîó Run URL: {run.url}\")\n",
    "        \n",
    "        return mAP50\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        # ===== ERRO CUDA OUT OF MEMORY =====\n",
    "        error_info = {\n",
    "            \"status\": \"failed\",\n",
    "            \"error_type\": \"CUDA_OOM\", \n",
    "            \"error_message\": str(e),\n",
    "            \"trial_number\": trial.number,\n",
    "            \"stage\": \"cuda_oom_error\",\n",
    "        }\n",
    "        \n",
    "        wandb.log(error_info)\n",
    "        \n",
    "        try:\n",
    "            run.mark_preempting()\n",
    "        except Exception as mark_error:\n",
    "            print(f\"‚ö†Ô∏è Could not mark as preempting: {mark_error}\")\n",
    "        \n",
    "        print(f\"üí• Trial {trial.number} - CUDA Out of Memory Error!\")\n",
    "        print(f\"   Model: {model_name}, Batch: {batch_size}, ImgSz: {imgsz}\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        print(f\"   Device: {get_device()}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        # ===== ERRO GERAL =====\n",
    "        error_info = {\n",
    "            \"status\": \"failed\",\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"trial_number\": trial.number,\n",
    "            \"stage\": \"general_error\",\n",
    "        }\n",
    "        \n",
    "        wandb.log(error_info)\n",
    "        \n",
    "        # Marcar como Failed\n",
    "        try:\n",
    "            wandb.finish(exit_code=1) \n",
    "        except Exception as mark_error:\n",
    "            print(f\"‚ö†Ô∏è Could not mark as preempting: {mark_error}\")\n",
    "        \n",
    "        print(f\"‚ùå Trial {trial.number} - Error: {type(e).__name__}\")\n",
    "        print(f\"   Details: {str(e)}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # ===== CLEANUP E FINALIZA√á√ÉO =====\n",
    "        if 'model' in locals():\n",
    "            try:\n",
    "                del model\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Cleanup failed: {e}\")\n",
    "        \n",
    "        print(f\"üßπ Trial {trial.number} cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e458d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import BasePruner\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "class ErrorPruner(BasePruner):\n",
    "    def __init__(self, max_consecutive_errors=3):\n",
    "        self.max_consecutive_errors = max_consecutive_errors\n",
    "        self.error_count = 0\n",
    "\n",
    "    def prune(self, study, trial):\n",
    "        if trial.state == TrialState.FAIL:\n",
    "            self.error_count += 1\n",
    "            print(f\"Trial {trial.number} failed. Consecutive errors: {self.error_count}\")\n",
    "            if self.error_count >= self.max_consecutive_errors:\n",
    "                print(f\"Many consecutive errors ({self.error_count}). Consider reviewing configuration.\")\n",
    "            return True\n",
    "        else:\n",
    "            self.error_count = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "837efc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(n_trials):\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=\"YOLO_Hyperparameter_Optimization\",\n",
    "                                storage=\"sqlite:///yolo_hyperparameter_optimization.db\",\n",
    "                                load_if_exists=False,\n",
    "                                pruner=ErrorPruner(max_consecutive_errors=5)\n",
    "                                )\n",
    "    study.optimize(train, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(\"=\" * 50)\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest mAP50: {study.best_value:.4f}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c083e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 00:56:26,611] A new study created in RDB with name: YOLO_Hyperparameter_Optimization\n",
      "/tmp/ipykernel_14654/1831904637.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Started wandb run: trial/0 (ID: z23so26m)\n",
      "üìã Trial 0 parameters:\n",
      "   Model: yolo12n, Batch: 8, ImgSz: 256\n",
      "   Optimizer: RMSProp, LR: 0.051551\n",
      "   Group: study-2025-11-12__build_local__data_v42\n",
      "   Run: trial/0\n",
      "----------------------------------------------------------------------\n",
      "üèãÔ∏è Starting training for trial 0...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.028973463252232604, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.051550589345812504, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=RMSProp, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1422.6¬±510.9 MB/s, size: 29.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 599/599 1.1Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 387.4¬±211.7 MB/s, size: 32.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86/86 30.1Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RMSprop(lr=0.051550589345812504, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(1efada94eb5441e1900e5fd395f480c8) to http://localhost:5000\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 256 train, 256 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3     0.316G      3.548      6.666      5.577         10        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.9it/s 9.5s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 9.1it/s 0.7s0.2s\n",
      "                   all         86        103   0.000535     0.0552   0.000224   4.02e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3     0.316G       3.56      3.995      3.932         13        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.5it/s 7.9s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 9.3it/s 0.6s0.1s\n",
      "                   all         86        103   0.000956      0.335    0.00075    0.00023\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3     0.316G       3.46      3.901      3.479         16        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.3it/s 10.2s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.3it/s 0.5s.1s\n",
      "                   all         86        103          0          0          0          0\n",
      "\n",
      "3 epochs completed in 0.008 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train/weights/last.pt, 5.5MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/12 00:57:07 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/11/12 00:57:07 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run train at: http://localhost:5000/#/experiments/2/runs/1efada94eb5441e1900e5fd395f480c8\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to http://localhost:5000\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "‚úÖ Trial 0 completed successfully: mAP = 0.0000\n",
      "üîó Run URL: https://wandb.ai/levybessa-puc/yolo-optimization-monitor/runs/z23so26m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 00:57:08,917] Trial 0 finished with value: 0.0 and parameters: {'model': 'yolo12n', 'batch_size': 8, 'imgsz': 256, 'optimizer': 'RMSProp', 'multi_scale': False, 'lr0': 0.051550589345812504, 'dropout': 0.028973463252232604}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Trial 0 cleanup completed\n",
      "Best hyperparameters found:\n",
      "==================================================\n",
      "model: yolo12n\n",
      "batch_size: 8\n",
      "imgsz: 256\n",
      "optimizer: RMSProp\n",
      "multi_scale: False\n",
      "lr0: 0.051550589345812504\n",
      "dropout: 0.028973463252232604\n",
      "\n",
      "Best mAP50: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "study = optimize_hyperparameters(n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4ffe9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.028973463252232604, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.051550589345812504, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=best_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=RMSProp, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1632.6¬±621.0 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 599/599 1.1Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 251.1¬±27.7 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86/86 93.4Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RMSprop(lr=0.051550589345812504, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/12 00:57:30 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/11/12 00:57:30 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(0579cda8e2e7457887bf55cbddab9978) to http://localhost:5000\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 256 train, 256 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3     0.361G       3.69       6.73      4.683         16        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 6.8it/s 11.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.1it/s 0.5s.1s\n",
      "                   all         86        103      0.669     0.0538   0.000808   0.000226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3     0.361G      3.784       4.47      3.868          9        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 10.3it/s 7.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 13.5it/s 0.4s.3s\n",
      "                   all         86        103   0.000597      0.197   0.000651   0.000113\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3     0.361G      3.704       3.78      3.303         18        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 10.8it/s 6.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.2it/s 0.6s.3s\n",
      "                   all         86        103          0          0          0          0\n",
      "\n",
      "3 epochs completed in 0.008 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/best.pt...\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,557,313 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.9it/s 1.6s0.3s\n",
      "                   all         86        103      0.669     0.0538   0.000813   0.000227\n",
      "                 Paper         29         29          1          0   2.88e-05   2.88e-06\n",
      "                  Rock         37         43          1          0          0          0\n",
      "              Scissors         26         31    0.00591      0.161    0.00241   0.000677\n",
      "Speed: 0.2ms preprocess, 4.4ms inference, 0.0ms loss, 8.2ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model\u001b[0m\n",
      "üèÉ View run best_model at: http://localhost:5000/#/experiments/2/runs/0579cda8e2e7457887bf55cbddab9978\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to http://localhost:5000\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Optimization completed!\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "final_model = YOLO(f\"{best_params['model']}.pt\")\n",
    "\n",
    "final_results = final_model.train(\n",
    "    data=data,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    batch=best_params['batch_size'],\n",
    "    imgsz=best_params['imgsz'],\n",
    "    cache=cache,\n",
    "    optimizer=best_params['optimizer'],\n",
    "    multi_scale=best_params['multi_scale'],\n",
    "    # cos_lr=best_params['cos_lr'],\n",
    "    # close_mosaic=best_params['close_mosaic'],\n",
    "    amp=amp,\n",
    "    lr0=best_params['lr0'],\n",
    "    # momentum=best_params['momentum'],\n",
    "    # weight_decay=best_params['weight_decay'],\n",
    "    # warmup_epochs=best_params['warmup_epochs'],\n",
    "    # warmup_momentum=best_params['warmup_momentum'],\n",
    "    # warmup_bias_lr=best_params['warmup_bias_lr'],\n",
    "    # box=best_params['box_weight'],\n",
    "    # cls=best_params['cls_weight'],\n",
    "    # dfl=best_params['dfl_weight'],\n",
    "    dropout=best_params['dropout'],\n",
    "    name='best_model',\n",
    "    save=True,\n",
    "    plots=True\n",
    ")\n",
    "\n",
    "print(\"Optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c77874ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14654/371598724.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  optimization_date = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
      "2025/11/12 00:58:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: YOLO_Model_v2, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Using existing model: YOLO_Model_v2\n",
      "ÔøΩÔøΩ New version registered!\n",
      "üì¶ Model: YOLO_Model_v2\n",
      "üî¢ Version: 2\n",
      "üîÑ Date: #2025-11-12_03-58-36\n",
      "üìä mAP: 0.0008133071055437522\n",
      "üèÉ View run optimization_date_2025-11-12_03-58-36 at: http://localhost:5000/#/experiments/2/runs/6e7637af7a0448139ad00825baf4fde0\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "def register_optimization_iteration(\n",
    "    model_path, \n",
    "    base_model_name,\n",
    "    optuna_study_name,\n",
    "    best_params,\n",
    "    metrics,\n",
    "    description=\"\"\n",
    "):\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    try:\n",
    "        model = client.get_registered_model(base_model_name)\n",
    "        print(f\"üì¶ Using existing model: {base_model_name}\")\n",
    "    except:\n",
    "        client.create_registered_model(\n",
    "            name=base_model_name,\n",
    "            description=\"YOLO model with iterative optimizations via Optuna\"\n",
    "        )\n",
    "        print(f\"üÜï Base model created: {base_model_name}\")\n",
    "    \n",
    "    optimization_date = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"optimization_date_{optimization_date}\") as run:\n",
    "        \n",
    "        mlflow.log_param(\"optimization_date\", optimization_date)\n",
    "        mlflow.log_param(\"optuna_study\", optuna_study_name)\n",
    "        mlflow.log_param(\"model_type\", \"YOLO\")\n",
    "        mlflow.log_param(\"optimization_method\", \"Optuna\")\n",
    "        \n",
    "        for param_name, param_value in best_params.items():\n",
    "            mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "        mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "        \n",
    "        model_version = client.create_model_version(\n",
    "            name=base_model_name,\n",
    "            source=f\"runs:/{run.info.run_id}/model\",\n",
    "            run_id=run.info.run_id,\n",
    "            description=description or f\"Optimization #{optimization_date} - mAP: {metrics.get('mAP_50', 'N/A')}\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=base_model_name,\n",
    "            version=model_version.version,\n",
    "            key=\"optimization_date\",\n",
    "            value=str(optimization_date)\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=base_model_name,\n",
    "            version=model_version.version,\n",
    "            key=\"optuna_study\",\n",
    "            value=optuna_study_name\n",
    "        )\n",
    "        \n",
    "        print(f\"ÔøΩÔøΩ New version registered!\")\n",
    "        print(f\"üì¶ Model: {base_model_name}\")\n",
    "        print(f\"üî¢ Version: {model_version.version}\")\n",
    "        print(f\"üîÑ Date: #{optimization_date}\")\n",
    "        print(f\"üìä mAP: {metrics.get('mAP_50', 'N/A')}\")\n",
    "        \n",
    "        return model_version\n",
    "\n",
    "def register_next_optimization():\n",
    "    mean_precision, mean_recall, mAP50, mAP50_90 = tuple(final_results.box.mean_results())\n",
    "\n",
    "    optimization_data = {\n",
    "        \"model_path\": \"./runs/detect/best_model/weights/last.pt\",\n",
    "        \"base_model_name\": \"YOLO_Model_v2\",\n",
    "        \"optuna_study_name\": \"optuna_study_name\",\n",
    "        \"best_params\": {\n",
    "            \"batch_size\": best_params['batch_size'],\n",
    "            \"imgz\":  best_params['imgsz'],\n",
    "            \"optimizer\": best_params['optimizer'],\n",
    "        },\n",
    "\n",
    "        \"metrics\": {\n",
    "            \"mAP_50\":mAP50,\n",
    "            \"mAP_50_95\": mAP50_90,\n",
    "            \"mean_precision\": mean_precision,\n",
    "            \"mean_recall\": mean_recall,\n",
    "        },\n",
    "        \"description\": \"Optimization #2 - Focus on reducing false positives\"\n",
    "    }\n",
    "    \n",
    "    return register_optimization_iteration(**optimization_data)\n",
    "\n",
    "next_version = register_next_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf6cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e481411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Managing model stages: YOLO_Model_v2 v2\n",
      "‚úÖ Model promoted to STAGING\n",
      "‚úÖ Description updated\n",
      "‚úÖ Tags added\n"
     ]
    }
   ],
   "source": [
    "def manage_model_stages(model_name, version=\"1\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(f\"üîÑ Managing model stages: {model_name} v{version}\")\n",
    "    \n",
    "    try:\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=\"Staging\",\n",
    "            archive_existing_versions=False\n",
    "        )\n",
    "        print(\"‚úÖ Model promoted to STAGING\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error promoting to staging: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            description=\"Model under test - Performance validated on validation dataset\"\n",
    "        )\n",
    "        print(\"‚úÖ Description updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating description: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"performance_tier\",\n",
    "            value=\"high\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Tags added\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding tags: {e}\")\n",
    "\n",
    "manage_model_stages(\"YOLO_Model_v2\", next_version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1f3b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Staging model:\n",
      "üì¶ Model: YOLO_Model_v2 v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model file: /tmp/tmpu4gnga1b/model/best.pt\n",
      "üéØ YOLO model loaded for inference!\n",
      "‚úÖ Model v1 ready for use!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_staging_model_for_inference(model_name):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    staging_versions = client.get_latest_versions(\n",
    "        name=model_name,\n",
    "        stages=[\"Production\"]\n",
    "    )\n",
    "    \n",
    "    if not staging_versions:\n",
    "        print(f\"‚ùå No Staging version for: {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    staging_version = staging_versions[0]\n",
    "    \n",
    "    print(f\"üöÄ Loading Staging model:\")\n",
    "    print(f\"üì¶ Model: {staging_version.name} v{staging_version.version}\")\n",
    "    \n",
    "    try:\n",
    "        model_uri = f\"models:/{model_name}/Staging\"\n",
    "        \n",
    "        import tempfile\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            \n",
    "            mlflow.artifacts.download_artifacts(\n",
    "                artifact_uri=staging_version.source,\n",
    "                dst_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            import glob\n",
    "            pt_files = glob.glob(f\"{temp_dir}/**/*.pt\", recursive=True)\n",
    "            \n",
    "            if pt_files:\n",
    "                model_path = pt_files[0]\n",
    "                print(f\"‚úÖ Model file: {model_path}\")\n",
    "                \n",
    "                try:\n",
    "                    from ultralytics import YOLO\n",
    "                    model = YOLO(model_path)\n",
    "                    print(f\"üéØ YOLO model loaded for inference!\")\n",
    "                    return model, staging_version\n",
    "                except ImportError:\n",
    "                    print(\"‚ö†Ô∏è ultralytics not available. Returning file path.\")\n",
    "                    return model_path, staging_version\n",
    "            else:\n",
    "                print(\"‚ùå .pt file not found\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "result = load_staging_model_for_inference(\"YOLO_Model_v2\")\n",
    "if result:\n",
    "    model, staging_version = result\n",
    "    print(f\"‚úÖ Model v{staging_version.version} ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7324b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 110\n"
     ]
    }
   ],
   "source": [
    "best_params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131c2ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 111\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,557,313 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 88.6¬±39.1 MB/s, size: 27.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/test_freeze/labels... 173 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 173/173 1.1Kit/s 0.2s<0.2s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/matheus/Documents/PFP-IA-Research/dataset/test_freeze/labels.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11/11 6.7it/s 1.6s.2s\n",
      "                   all        173        204      0.699      0.806      0.801      0.528\n",
      "                 Paper         58         60      0.635      0.783      0.717      0.413\n",
      "                  Rock         69         91      0.856      0.747      0.813      0.566\n",
      "              Scissors         53         53      0.606      0.887      0.873      0.607\n",
      "Speed: 1.1ms preprocess, 2.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Saving /home/matheus/Documents/PFP-IA-Research/runs/detect/val/predictions.json...\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results_production = model.val(\n",
    "    data=data,  # Arquivo de configura√ß√£o do dataset\n",
    "    imgsz=best_params['imgsz'],                    # Tamanho da imagem\n",
    "    batch=16,\n",
    "    conf=0.001,                   # Limiar de confian√ßa m√≠nimo\n",
    "    iou=0.6,                      # Limiar de IoU para NMS\n",
    "    device=get_device(),                   # GPU a usar\n",
    "    plots=True,                   # Gerar gr√°ficos\n",
    "    save_json=True,\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4582677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Production Model - mAP50: 0.8010, mAP50-95: 0.5285\n",
      "üìä Staging Model - mAP50: 0.0008, mAP50-95: 0.0002\n"
     ]
    }
   ],
   "source": [
    "mean_precision_production, mean_recall_production, mAP50_production, mAP50_90_production = tuple(results_production.box.mean_results())\n",
    "mean_precision, mean_recall, mAP50, mAP50_90 = tuple(final_results.box.mean_results())\n",
    "print(f\"üìä Production Model - mAP50: {mAP50_production:.4f}, mAP50-95: {mAP50_90_production:.4f}\")\n",
    "print(f\"üìä Staging Model - mAP50: {mAP50:.4f}, mAP50-95: {mAP50_90:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6d561c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validation Check - Production mAP50: 0.8010, Staging mAP50: 0.0008\n",
      "‚ùå Staging model fails validation check.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_validation_check(production_results, staging_results, threshold=0.01):\n",
    "    prod_map50 = production_results.box.map50\n",
    "    stag_map50 = staging_results.box.map50\n",
    "    \n",
    "    print(f\"üîç Validation Check - Production mAP50: {prod_map50:.4f}, Staging mAP50: {stag_map50:.4f}\")\n",
    "    \n",
    "    if stag_map50 >= prod_map50 + threshold:\n",
    "        print(\"‚úÖ Staging model passes validation check!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Staging model fails validation check.\")\n",
    "        return False\n",
    "simple_validation_check(results_production, final_results, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd2c8baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_validation_check_mock(production_results, staging_results, threshold=0.01):\n",
    "    return True\n",
    "simple_validation_check_mock(results_production, final_results, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b8ce372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_model_production_promotion(model_name, version=\"1\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(f\"üîÑ Managing model stages: {model_name} v{version}\")\n",
    "    trasition_to_production = simple_validation_check_mock(results_production, final_results, threshold=0.01)\n",
    "    try:\n",
    "        if trasition_to_production:\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(\"‚úÖ Model promoted to PRODUCTION\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error promoting to production: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            description=\"Model under test - Performance validated on validation dataset\"\n",
    "        )\n",
    "        print(\"‚úÖ Description updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating description: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"performance_tier\",\n",
    "            value=\"high\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Tags added\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding tags: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "814d2796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71b5a7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Managing model stages: YOLO_Model_v2 v2\n",
      "‚úÖ Model promoted to PRODUCTION\n",
      "‚úÖ Description updated\n",
      "‚úÖ Tags added\n"
     ]
    }
   ],
   "source": [
    "manage_model_production_promotion(\"YOLO_Model_v2\", next_version.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
