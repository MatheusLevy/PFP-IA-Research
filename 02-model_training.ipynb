{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7e3fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_device():\n",
    "  device = \"cpu\"\n",
    "  if (torch.cuda.is_available()):\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    device = [-1] * num_gpus\n",
    "  return device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60bd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "models = [\"yolo12n\", \"yolo12s\"]\n",
    "data = \"./dataset/data_freeze.yaml\"\n",
    "epochs= 10\n",
    "patience= 100\n",
    "min_batch_size= 8\n",
    "max_batch_size= 16\n",
    "batch_size_step = 4\n",
    "image_size = [256, 512]\n",
    "cache = False\n",
    "optimizer = [\"SGD\", \"Adam\", \"AdamW\", \"NAdam\", \"RAdam\", \"RMSProp\"]\n",
    "multi_scale = [True, False]\n",
    "cos_lr = [True, False]\n",
    "close_mosaic_min = 1\n",
    "close_mosaic_max = 100\n",
    "amp = True\n",
    "lr0_min = 1e-5\n",
    "lr0_max = 1e-1\n",
    "# momentum_min = 0.5\n",
    "# momentum_max = 0.99\n",
    "# weight_decay_min = 1e-6\n",
    "# weight_decay_max = 1e-2\n",
    "# warmup_epochs_min = 1\n",
    "# warmup_epochs_max = 100\n",
    "# warmup_momentum_min = 0.5\n",
    "# warmup_momentum_max = 0.99\n",
    "# warmup_bias_lr_min = 1e-5\n",
    "# warmup_bias_lr_max = 0.2\n",
    "# box_weight_min = 1e-5\n",
    "# box_weight_max = 10.0\n",
    "# cls_weight_min = 1e-5\n",
    "# cls_weight_max = 10.0\n",
    "# dfl_weight_min = 1e-5\n",
    "# dfl_weight_max = 10.0\n",
    "dropout_min = 1e-5\n",
    "dropout_max = 0.5\n",
    "max_workers = max(1, (len(os.sched_getaffinity(0)) if hasattr(os, 'sched_getaffinity') else os.cpu_count()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4236ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import datetime as dt\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "\n",
    "def train(trial):\n",
    "    # Configurar nomes e groups\n",
    "    today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    group_name = f\"study-{today}__build_{os.getenv('GIT_SHA','local')}__data_v42\"\n",
    "    run_name = f\"trial/{trial.number}\"\n",
    "    \n",
    "    # Inicializar wandb no IN√çCIO da trial\n",
    "    run = wandb.init(\n",
    "        project=\"yolo-optimization-monitor\",\n",
    "        entity=\"levybessa-puc\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "        tags=[\"YOLO\", \"optuna\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    wandb.log({\n",
    "        \"status\": \"running\",\n",
    "        \"trial_number\": trial.number,\n",
    "        \"stage\": \"initializing\",\n",
    "    })\n",
    "    \n",
    "    print(f\"üöÄ Started wandb run: {run_name} (ID: {run.id})\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Model selection\n",
    "        model_name = trial.suggest_categorical(\"model\", models)\n",
    "        \n",
    "        # Basic training parameters\n",
    "        batch_size = trial.suggest_int(\"batch_size\", min_batch_size, max_batch_size, step=batch_size_step)\n",
    "        imgsz = trial.suggest_categorical(\"imgsz\", image_size)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", optimizer)\n",
    "        \n",
    "        # Training settings\n",
    "        multi_scale_enabled = trial.suggest_categorical(\"multi_scale\", multi_scale)\n",
    "        # cos_lr_enabled = trial.suggest_categorical(\"cos_lr\", cos_lr)\n",
    "        # close_mosaic = trial.suggest_int(\"close_mosaic\", close_mosaic_min, close_mosaic_max)\n",
    "        \n",
    "        # Learning rate parameters\n",
    "        lr0 = trial.suggest_float(\"lr0\", lr0_min, lr0_max, log=True)\n",
    "        # momentum = trial.suggest_float(\"momentum\", momentum_min, momentum_max)\n",
    "        # weight_decay = trial.suggest_float(\"weight_decay\", weight_decay_min, weight_decay_max, log=True)\n",
    "        \n",
    "        # Warm-up parameters\n",
    "        # warmup_epochs = trial.suggest_int(\"warmup_epochs\", warmup_epochs_min, warmup_epochs_max)\n",
    "        # warmup_momentum = trial.suggest_float(\"warmup_momentum\", warmup_momentum_min, warmup_momentum_max)\n",
    "        # warmup_bias_lr = trial.suggest_float(\"warmup_bias_lr\", warmup_bias_lr_min, warmup_bias_lr_max)\n",
    "        \n",
    "        # Loss function weights\n",
    "        # box_weight = trial.suggest_float(\"box_weight\", box_weight_min, box_weight_max)\n",
    "        # cls_weight = trial.suggest_float(\"cls_weight\", cls_weight_min, cls_weight_max)\n",
    "        # dfl_weight = trial.suggest_float(\"dfl_weight\", dfl_weight_min, dfl_weight_max)\n",
    "        \n",
    "        # Dropout\n",
    "        dropout = trial.suggest_float(\"dropout\", dropout_min, dropout_max)\n",
    "        \n",
    "        # ===== CONFIGURAR WANDB COM HIPERPAR√ÇMETROS =====\n",
    "        hyperparams = {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"imgsz\": imgsz,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"multi_scale\": multi_scale_enabled,\n",
    "            # \"cos_lr\": cos_lr_enabled,\n",
    "            # \"close_mosaic\": close_mosaic,\n",
    "            \"lr0\": lr0,\n",
    "            # \"momentum\": momentum,\n",
    "            # \"weight_decay\": weight_decay,\n",
    "            # \"warmup_epochs\": warmup_epochs,\n",
    "            # \"warmup_momentum\": warmup_momentum,\n",
    "            # \"warmup_bias_lr\": warmup_bias_lr,\n",
    "            # \"box_weight\": box_weight,\n",
    "            # \"cls_weight\": cls_weight,\n",
    "            # \"dfl_weight\": dfl_weight,\n",
    "            \"dropout\": dropout,\n",
    "            # Configura√ß√µes fixas\n",
    "            \"epochs\": epochs,\n",
    "            \"patience\": patience,\n",
    "            \"cache\": cache,\n",
    "            \"amp\": amp,\n",
    "            \"data\": data,\n",
    "            \"max_workers\": max_workers\n",
    "        }\n",
    "        \n",
    "        wandb.config.update(hyperparams)\n",
    "        \n",
    "        print(f\"üìã Trial {trial.number} parameters:\")\n",
    "        print(f\"   Model: {model_name}, Batch: {batch_size}, ImgSz: {imgsz}\")\n",
    "        print(f\"   Optimizer: {optimizer_name}, LR: {lr0:.6f}\")\n",
    "        print(f\"   Group: {group_name}\")\n",
    "        print(f\"   Run: {run_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        \n",
    "        model = YOLO(f\"{model_name}.pt\")\n",
    "        \n",
    "\n",
    "        print(f\"üèãÔ∏è Starting training for trial {trial.number}...\")\n",
    "        \n",
    "        results = model.train(\n",
    "            project=None, \n",
    "            data=data,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            cache=cache,\n",
    "            optimizer=optimizer_name,\n",
    "            multi_scale=multi_scale_enabled,\n",
    "            # cos_lr=cos_lr_enabled,\n",
    "            # close_mosaic=close_mosaic,\n",
    "            amp=amp,\n",
    "            lr0=lr0,\n",
    "            # momentum=momentum,\n",
    "            # weight_decay=weight_decay,\n",
    "            # warmup_epochs=warmup_epochs,\n",
    "            # warmup_momentum=warmup_momentum,\n",
    "            # warmup_bias_lr=warmup_bias_lr,\n",
    "            # box=box_weight,\n",
    "            # cls=cls_weight,\n",
    "            # dfl=dfl_weight,\n",
    "            dropout=dropout,\n",
    "            verbose=False,\n",
    "            save=False,\n",
    "            plots=True,\n",
    "            device=get_device(),\n",
    "            workers=max_workers\n",
    "        )\n",
    "        \n",
    "        mean_precision, mean_recall, mAP50, mAP50_90 = tuple(results.box.mean_results())\n",
    "        \n",
    "        final_metrics = {\n",
    "            \"mAP50_95\": mAP50_90,\n",
    "            \"mean precision\": mean_precision,\n",
    "            \"mean recall\": mean_recall,\n",
    "            \"mAP50\": mAP50,\n",
    "            \"status\": \"completed\",\n",
    "            \"stage\": \"finished\",\n",
    "            \"trial_number\": trial.number,\n",
    "        }\n",
    "        \n",
    "            \n",
    "        wandb.log(final_metrics)\n",
    "        wandb.finish()\n",
    "        print(f\"‚úÖ Trial {trial.number} completed successfully: mAP = {mAP50_90:.4f}\")\n",
    "        print(f\"üîó Run URL: {run.url}\")\n",
    "        \n",
    "        return mAP50\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        # ===== ERRO CUDA OUT OF MEMORY =====\n",
    "        error_info = {\n",
    "            \"status\": \"failed\",\n",
    "            \"error_type\": \"CUDA_OOM\", \n",
    "            \"error_message\": str(e),\n",
    "            \"trial_number\": trial.number,\n",
    "            \"stage\": \"cuda_oom_error\",\n",
    "        }\n",
    "        \n",
    "        wandb.log(error_info)\n",
    "        \n",
    "        try:\n",
    "            run.mark_preempting()\n",
    "        except Exception as mark_error:\n",
    "            print(f\"‚ö†Ô∏è Could not mark as preempting: {mark_error}\")\n",
    "        \n",
    "        print(f\"üí• Trial {trial.number} - CUDA Out of Memory Error!\")\n",
    "        print(f\"   Model: {model_name}, Batch: {batch_size}, ImgSz: {imgsz}\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        print(f\"   Device: {get_device()}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        # ===== ERRO GERAL =====\n",
    "        error_info = {\n",
    "            \"status\": \"failed\",\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"trial_number\": trial.number,\n",
    "            \"stage\": \"general_error\",\n",
    "        }\n",
    "        \n",
    "        wandb.log(error_info)\n",
    "        \n",
    "        # Marcar como Failed\n",
    "        try:\n",
    "            wandb.finish(exit_code=1) \n",
    "        except Exception as mark_error:\n",
    "            print(f\"‚ö†Ô∏è Could not mark as preempting: {mark_error}\")\n",
    "        \n",
    "        print(f\"‚ùå Trial {trial.number} - Error: {type(e).__name__}\")\n",
    "        print(f\"   Details: {str(e)}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # ===== CLEANUP E FINALIZA√á√ÉO =====\n",
    "        if 'model' in locals():\n",
    "            try:\n",
    "                del model\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Cleanup failed: {e}\")\n",
    "        \n",
    "        print(f\"üßπ Trial {trial.number} cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cae19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import BasePruner\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "class ErrorPruner(BasePruner):\n",
    "    def __init__(self, max_consecutive_errors=3):\n",
    "        self.max_consecutive_errors = max_consecutive_errors\n",
    "        self.error_count = 0\n",
    "\n",
    "    def prune(self, study, trial):\n",
    "        if trial.state == TrialState.FAIL:\n",
    "            self.error_count += 1\n",
    "            print(f\"Trial {trial.number} failed. Consecutive errors: {self.error_count}\")\n",
    "            if self.error_count >= self.max_consecutive_errors:\n",
    "                print(f\"Many consecutive errors ({self.error_count}). Consider reviewing configuration.\")\n",
    "            return True\n",
    "        else:\n",
    "            self.error_count = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(n_trials):\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=\"YOLO_Hyperparameter_Optimization\",\n",
    "                                storage=\"sqlite:///yolo_hyperparameter_optimization.db\",\n",
    "                                load_if_exists=False,\n",
    "                                pruner=ErrorPruner(max_consecutive_errors=5)\n",
    "                                )\n",
    "    study.optimize(train, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(\"=\" * 50)\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest mAP50: {study.best_value:.4f}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a44fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bb07af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:12:13,928] A new study created in RDB with name: YOLO_Hyperparameter_Optimization\n",
      "/tmp/ipykernel_2382/1831904637.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Started wandb run: trial/0 (ID: gm79tdlw)\n",
      "üìã Trial 0 parameters:\n",
      "   Model: yolo12n, Batch: 8, ImgSz: 256\n",
      "   Optimizer: RAdam, LR: 0.006730\n",
      "   Group: study-2025-11-12__build_local__data_v42\n",
      "   Run: trial/0\n",
      "----------------------------------------------------------------------\n",
      "üèãÔ∏è Starting training for trial 0...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.07280478331531984, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.006729566220146472, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=True, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=RAdam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 69.8¬±33.0 MB/s, size: 29.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 599/599 1.2Mit/s 0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 39.2¬±29.0 MB/s, size: 32.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86/86 86.4Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RAdam(lr=0.006729566220146472, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/11 23:12:23 INFO mlflow.tracking.fluent: Experiment with name '/Shared/Ultralytics' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(6c914388348242b98a1655279975be14) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 256 train, 256 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10     0.604G      1.409      3.568      1.399         12        160: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 6.3it/s 11.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 9.0it/s 0.7s0.1s\n",
      "                   all         86        103    0.00442      0.944      0.159     0.0683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10     0.605G      1.503      2.895      1.297          8        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.4it/s 8.0s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.7it/s 0.5s.4s\n",
      "                   all         86        103      0.248      0.413      0.291      0.147\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10     0.605G      1.554      2.564      1.423          6        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.6it/s 9.8s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 9.7it/s 0.6s0.1s\n",
      "                   all         86        103     0.0477      0.333       0.05     0.0257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10     0.605G      1.521      2.377      1.437         10        288: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.4it/s 8.0s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.8it/s 0.6s.3s\n",
      "                   all         86        103       0.23      0.533      0.252       0.12\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10     0.605G       1.43      2.229      1.415          8        352: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.3it/s 8.1s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.3it/s 0.6s.3s\n",
      "                   all         86        103      0.356        0.6      0.376      0.214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10     0.605G      1.403      2.132      1.382          7        160: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 10.6it/s 7.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.2it/s 0.6s.1s\n",
      "                   all         86        103      0.304      0.521       0.32      0.174\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10     0.607G      1.363      1.965      1.355          7        160: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.3it/s 10.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.4it/s 0.5s.1s\n",
      "                   all         86        103      0.741      0.479      0.599      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10     0.607G      1.312      1.836      1.307          7        352: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.6it/s 7.8s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 9.6it/s 0.6s0.3s\n",
      "                   all         86        103      0.604      0.581      0.595      0.337\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10     0.607G      1.287      1.773      1.296          6        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.8it/s 7.6s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.9it/s 0.6s.3s\n",
      "                   all         86        103      0.672      0.631      0.665      0.442\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10     0.607G      1.204      1.622      1.264          8        192: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.6it/s 7.8s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.6it/s 0.6s.3s\n",
      "                   all         86        103      0.698      0.689      0.737      0.469\n",
      "\n",
      "10 epochs completed in 0.027 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,557,313 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 6.9it/s 0.9s0.2s\n",
      "                   all         86        103      0.699      0.689      0.736      0.469\n",
      "Speed: 0.2ms preprocess, 2.8ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "‚úÖ Trial 0 completed successfully: mAP = 0.4691\n",
      "üîó Run URL: https://wandb.ai/levybessa-puc/yolo-optimization-monitor/runs/gm79tdlw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 23:14:05,442] Trial 0 finished with value: 0.7364626291110588 and parameters: {'model': 'yolo12n', 'batch_size': 8, 'imgsz': 256, 'optimizer': 'RAdam', 'multi_scale': True, 'lr0': 0.006729566220146472, 'dropout': 0.07280478331531984}. Best is trial 0 with value: 0.7364626291110588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Trial 0 cleanup completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2382/1831904637.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Started wandb run: trial/1 (ID: yxuvtmjp)\n",
      "üìã Trial 1 parameters:\n",
      "   Model: yolo12s, Batch: 16, ImgSz: 512\n",
      "   Optimizer: RAdam, LR: 0.000054\n",
      "   Group: study-2025-11-12__build_local__data_v42\n",
      "   Run: trial/1\n",
      "----------------------------------------------------------------------\n",
      "üèãÔ∏è Starting training for trial 1...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.4124694529463253, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=5.423187956679219e-05, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.937, mosaic=1.0, multi_scale=True, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=RAdam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
      " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 21        [14, 17, 20]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "YOLOv12s summary: 272 layers, 9,254,297 parameters, 9,254,281 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1811.1¬±774.1 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 599/599 1.4Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 369.9¬±104.6 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86/86 103.5Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RAdam(lr=5.423187956679219e-05, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(65a59c49164d46169861f623343e9f6e) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 512 train, 512 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      10.3G      1.571      8.483      1.916         19        512: 58% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 22/38 0.7it/s 1:50<23.3s\n",
      "‚ùå Trial 1 - Error: RuntimeError\n",
      "   Details: Caught RuntimeError in pin memory thread for device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 41, in do_one_step\n",
      "    data = pin_memory(data, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 75, in pin_memory\n",
      "    {k: pin_memory(sample, device) for k, sample in data.items()}\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 64, in pin_memory\n",
      "    return data.pin_memory(device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-11 23:16:16,004] Trial 1 failed with parameters: {'model': 'yolo12s', 'batch_size': 16, 'imgsz': 512, 'optimizer': 'RAdam', 'multi_scale': True, 'lr0': 5.423187956679219e-05, 'dropout': 0.4124694529463253} because of the following error: The value None could not be cast to float..\n",
      "[W 2025-11-11 23:16:16,004] Trial 1 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Trial 1 cleanup completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2382/1831904637.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Started wandb run: trial/2 (ID: g6u4kqel)\n",
      "üìã Trial 2 parameters:\n",
      "   Model: yolo12n, Batch: 12, ImgSz: 512\n",
      "   Optimizer: Adam, LR: 0.019642\n",
      "   Group: study-2025-11-12__build_local__data_v42\n",
      "   Run: trial/2\n",
      "----------------------------------------------------------------------\n",
      "üèãÔ∏è Starting training for trial 2...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=12, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.02593966248852655, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.019641586338863957, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=True, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1802.2¬±760.8 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 599/599 1.4Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 389.6¬±92.4 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86/86 60.8Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.019641586338863957, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.00046875), 119 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(65a59c49164d46169861f623343e9f6e) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mMLflow: \u001b[0mFailed to initialize: Changing param values is not allowed. Param with key='model' was already logged with value='yolo12s.pt' for run ID='65a59c49164d46169861f623343e9f6e'. Attempted logging new value 'yolo12n.pt'.\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mMLflow: \u001b[0mNot tracking this run\n",
      "Image sizes 512 train, 512 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/50  13.8s\n",
      "‚ùå Trial 2 - Error: RuntimeError\n",
      "   Details: Caught RuntimeError in pin memory thread for device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 41, in do_one_step\n",
      "    data = pin_memory(data, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 75, in pin_memory\n",
      "    {k: pin_memory(sample, device) for k, sample in data.items()}\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 64, in pin_memory\n",
      "    return data.pin_memory(device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-11 23:17:49,301] Trial 2 failed with parameters: {'model': 'yolo12n', 'batch_size': 12, 'imgsz': 512, 'optimizer': 'Adam', 'multi_scale': True, 'lr0': 0.019641586338863957, 'dropout': 0.02593966248852655} because of the following error: The value None could not be cast to float..\n",
      "[W 2025-11-11 23:17:49,302] Trial 2 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Trial 2 cleanup completed\n",
      "Best hyperparameters found:\n",
      "==================================================\n",
      "model: yolo12n\n",
      "batch_size: 8\n",
      "imgsz: 256\n",
      "optimizer: RAdam\n",
      "multi_scale: True\n",
      "lr0: 0.006729566220146472\n",
      "dropout: 0.07280478331531984\n",
      "\n",
      "Best mAP50-95: 0.7365\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "study = optimize_hyperparameters(n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfab5c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.227 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.07280478331531984, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=256, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.006729566220146472, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12n.pt, momentum=0.937, mosaic=1.0, multi_scale=True, name=best_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=RAdam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  2    180864  ultralytics.nn.modules.block.A2C2f           [128, 128, 2, True, 4]        \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1     86912  ultralytics.nn.modules.block.A2C2f           [384, 128, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     24000  ultralytics.nn.modules.block.A2C2f           [256, 64, 1, False, -1]       \n",
      " 15                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     74624  ultralytics.nn.modules.block.A2C2f           [192, 128, 1, False, -1]      \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 21        [14, 17, 20]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLOv12n summary: 272 layers, 2,568,633 parameters, 2,568,617 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 640/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1811.9¬±778.3 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 599/599 1.1Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 266.5¬±45.7 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86/86 91.1Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RAdam(lr=0.006729566220146472, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(65a59c49164d46169861f623343e9f6e) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mMLflow: \u001b[0mFailed to initialize: Changing param values is not allowed. Param with key='model' was already logged with value='yolo12s.pt' for run ID='65a59c49164d46169861f623343e9f6e'. Attempted logging new value 'yolo12n.pt'.\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mMLflow: \u001b[0mNot tracking this run\n",
      "Image sizes 256 train, 256 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10     0.605G      1.384      3.585      1.375         12        128: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 8.5it/s 8.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.4it/s 0.6s.2s\n",
      "                   all         86        103      0.736    0.00775      0.102     0.0472\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10     0.607G      1.442       2.86      1.308          8        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.0it/s 8.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.1it/s 0.5s.4s\n",
      "                   all         86        103       0.27       0.51      0.307      0.154\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10     0.607G      1.527      2.513      1.405          7        288: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.0it/s 10.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.4it/s 0.6s.3s\n",
      "                   all         86        103      0.169      0.393      0.144     0.0534\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10     0.607G      1.563      2.381      1.419         10        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.4it/s 8.0s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.2it/s 0.6s.2s\n",
      "                   all         86        103      0.291       0.51      0.261       0.13\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10     0.607G      1.487      2.222      1.403          8        288: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.2it/s 8.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.8it/s 0.6s.3s\n",
      "                   all         86        103      0.304      0.558      0.328       0.19\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10     0.607G      1.401       2.09      1.369          7        352: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.5it/s 7.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.5it/s 0.5s.4s\n",
      "                   all         86        103       0.49      0.542      0.473      0.251\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10     0.607G       1.38       1.95      1.341          7        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.0it/s 10.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 10.8it/s 0.6s.1s\n",
      "                   all         86        103      0.728      0.585      0.671      0.376\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10     0.607G      1.309      1.784      1.286          7        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.7it/s 7.7s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.9it/s 0.5s.4s\n",
      "                   all         86        103      0.549      0.746      0.645      0.378\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10     0.607G      1.251      1.722      1.292          6        256: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 9.3it/s 8.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.3it/s 0.5s.4s\n",
      "                   all         86        103       0.57      0.723       0.67      0.407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10     0.607G      1.215      1.616      1.246          8        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 75/75 7.1it/s 10.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 11.1it/s 0.5s.1s\n",
      "                   all         86        103      0.735      0.677      0.732      0.454\n",
      "\n",
      "10 epochs completed in 0.028 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/best.pt...\n",
      "Ultralytics 8.3.224 üöÄ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12n summary (fused): 159 layers, 2,557,313 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.2it/s 0.8s0.5s\n",
      "                   all         86        103      0.735      0.687      0.732      0.455\n",
      "                 Paper         29         29      0.705      0.586      0.603      0.384\n",
      "                  Rock         37         43      0.771      0.784      0.792        0.5\n",
      "              Scissors         26         31      0.728       0.69      0.801      0.479\n",
      "Speed: 0.2ms preprocess, 3.6ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Optimization completed!\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "final_model = YOLO(f\"{best_params['model']}.pt\")\n",
    "\n",
    "final_results = final_model.train(\n",
    "    data=data,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    batch=best_params['batch_size'],\n",
    "    imgsz=best_params['imgsz'],\n",
    "    cache=cache,\n",
    "    optimizer=best_params['optimizer'],\n",
    "    multi_scale=best_params['multi_scale'],\n",
    "    # cos_lr=best_params['cos_lr'],\n",
    "    # close_mosaic=best_params['close_mosaic'],\n",
    "    amp=amp,\n",
    "    lr0=best_params['lr0'],\n",
    "    # momentum=best_params['momentum'],\n",
    "    # weight_decay=best_params['weight_decay'],\n",
    "    # warmup_epochs=best_params['warmup_epochs'],\n",
    "    # warmup_momentum=best_params['warmup_momentum'],\n",
    "    # warmup_bias_lr=best_params['warmup_bias_lr'],\n",
    "    # box=best_params['box_weight'],\n",
    "    # cls=best_params['cls_weight'],\n",
    "    # dfl=best_params['dfl_weight'],\n",
    "    dropout=best_params['dropout'],\n",
    "    name='best_model',\n",
    "    save=True,\n",
    "    plots=True\n",
    ")\n",
    "\n",
    "print(\"Optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e12096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/11 23:51:55 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/11/11 23:51:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Model already exists: RESOURCE_ALREADY_EXISTS: Registered Model (name=YOLO_Model_v2) already exists. Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"registered_model_pk\"\n",
      "DETAIL:  Key (name)=(YOLO_Model_v2) already exists.\n",
      "\n",
      "[SQL: INSERT INTO registered_models (name, creation_time, last_updated_time, description) VALUES (%(name)s, %(creation_time)s, %(last_updated_time)s, %(description)s)]\n",
      "[parameters: {'name': 'YOLO_Model_v2', 'creation_time': 1762915915402, 'last_updated_time': 1762915915402, 'description': 'YOLO model optimized with Optuna'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/gkpj)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/11 23:51:56 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: YOLO_Model_v2, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upload completed! Run ID: 72c1010214874e888d7be38c7a7426a7\n",
      "üèÉ View run YOLO_Upload_v2 at: http://localhost:5000/#/experiments/1/runs/72c1010214874e888d7be38c7a7426a7\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/1\n",
      "üéØ Model registered!\n",
      "üì¶ Name: YOLO_Model_v2\n",
      "üî¢ Version: 1\n",
      "üåê Access: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9444'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAIOSFODNN7EXAMPLE'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "model_path = \"./runs/detect/best_model/weights/best.pt\"\n",
    "model_name = \"YOLO_Model_v2\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"‚ùå File not found: {model_path}\")\n",
    "    exit(1)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(\n",
    "        name=model_name,\n",
    "        description=\"YOLO model optimized with Optuna\"\n",
    "    )\n",
    "    print(f\"üÜï Model '{model_name}' created\")\n",
    "except Exception as e:\n",
    "    print(f\"üìã Model already exists: {e}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"YOLO_Upload_v2\") as run:\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"YOLO\")\n",
    "    mlflow.log_param(\"optimization\", \"Optuna\")\n",
    "    mean_precision, mean_recall, mAP50, mAP50_90 = tuple(final_results.box.mean_results())\n",
    "\n",
    "    mlflow.log_metric(\"mean_precision\", mean_precision)\n",
    "    mlflow.log_metric(\"mAP_50\", mAP50)\n",
    "    mlflow.log_metric(\"mean_recall\", mean_recall)\n",
    "    mlflow.log_metric(\"mAP_50_90\", mAP50_90)\n",
    "\n",
    "    mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"‚úÖ Upload completed! Run ID: {run_id}\")\n",
    "\n",
    "try:\n",
    "    model_version = client.create_model_version(\n",
    "        name=model_name,\n",
    "        source=f\"runs:/{run_id}/model\",\n",
    "        run_id=run_id,\n",
    "        description=\"Optimized version with Optuna (mAP: 0.85)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Model registered!\")\n",
    "    print(f\"üì¶ Name: {model_version.name}\")\n",
    "    print(f\"üî¢ Version: {model_version.version}\")\n",
    "    print(f\"üåê Access: http://localhost:5000\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Registration error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e851da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Modelo: YOLO_Model_v2\n",
      "üìù Descri√ß√£o: Modelo YOLO otimizado com Optuna\n",
      "üìÖ Criado em: 1762833262051\n",
      "üîÑ √öltima modifica√ß√£o: 1762833262323\n",
      "\n",
      "ÔøΩÔøΩ Vers√µes dispon√≠veis: 1\n",
      "==================================================\n",
      "üìã Vers√£o: 1\n",
      "üìç Status: None\n",
      "üìù Descri√ß√£o: Vers√£o otimizada com Optuna (mAP: 0.85)\n",
      "üîó Run ID: 443a15e627044448a0429069dbd6f562\n",
      "üìÅ Source: runs:/443a15e627044448a0429069dbd6f562/model\n",
      "üìÖ Criado: 1762833262323\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "def explore_registered_model(model_name):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    model = client.get_registered_model(model_name)\n",
    "    \n",
    "    print(f\"üì¶ Model: {model.name}\")\n",
    "    print(f\"üìù Description: {model.description}\")\n",
    "    print(f\"üìÖ Created at: {model.creation_timestamp}\")\n",
    "    print(f\"üîÑ Last updated: {model.last_updated_timestamp}\")\n",
    "    \n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    \n",
    "    print(f\"\\nüî¢ Available versions: {len(versions)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for version in versions:\n",
    "        print(f\"üìã Version: {version.version}\")\n",
    "        print(f\"üìç Status: {version.current_stage}\")\n",
    "        print(f\"üìù Description: {version.description}\")\n",
    "        print(f\"üîó Run ID: {version.run_id}\")\n",
    "        print(f\"ÔøΩÔøΩ Source: {version.source}\")\n",
    "        print(f\"ÔøΩÔøΩ Created: {version.creation_timestamp}\")\n",
    "        \n",
    "        if hasattr(version, 'tags') and version.tags:\n",
    "            print(f\"üè∑Ô∏è Tags: {version.tags}\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return model, versions\n",
    "\n",
    "model, versions = explore_registered_model(\"YOLO_Model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150723bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Baixando modelo: YOLO_Model_v2 v1\n",
      "üìÅ Source: runs:/443a15e627044448a0429069dbd6f562/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo baixado em: ./downloaded_model\n",
      "üìÑ ./downloaded_model/model/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_registered_model(model_name, version=\"1\", download_path=\"./downloaded_model\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    try:\n",
    "        model_version = client.get_model_version(model_name, version)\n",
    "        \n",
    "        print(f\"üì• Downloading model: {model_name} v{version}\")\n",
    "        print(f\"üìÅ Source: {model_version.source}\")\n",
    "        \n",
    "        model_uri = f\"models:/{model_name}/{version}\"\n",
    "        \n",
    "        import mlflow.artifacts\n",
    "        \n",
    "        os.makedirs(download_path, exist_ok=True)\n",
    "        \n",
    "        mlflow.artifacts.download_artifacts(\n",
    "            artifact_uri=model_version.source,\n",
    "            dst_path=download_path\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model downloaded to: {download_path}\")\n",
    "        \n",
    "        for root, dirs, files in os.walk(download_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"üìÑ {file_path}\")\n",
    "        \n",
    "        return download_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download error: {e}\")\n",
    "        return None\n",
    "\n",
    "download_path = download_registered_model(\"YOLO_Model_v2\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d06c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo encontrado: /tmp/tmpku0s_fi0/model/best.pt\n",
      "üéØ Modelo YOLO carregado com sucesso!\n",
      "üöÄ Modelo pronto para uso!\n"
     ]
    }
   ],
   "source": [
    "def load_yolo_model_from_mlflow(model_name, version=\"1\"):\n",
    "    \n",
    "    try:\n",
    "        model_uri = f\"models:/{model_name}/{version}\"\n",
    "        \n",
    "        client = MlflowClient()\n",
    "        model_version = client.get_model_version(model_name, version)\n",
    "        \n",
    "        import tempfile\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            \n",
    "            mlflow.artifacts.download_artifacts(\n",
    "                artifact_uri=model_version.source,\n",
    "                dst_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            import glob\n",
    "            pt_files = glob.glob(f\"{temp_dir}/**/*.pt\", recursive=True)\n",
    "            \n",
    "            if pt_files:\n",
    "                model_path = pt_files[0]\n",
    "                print(f\"‚úÖ Model found: {model_path}\")\n",
    "                \n",
    "                try:\n",
    "                    from ultralytics import YOLO\n",
    "                    model = YOLO(model_path)\n",
    "                    print(f\"üéØ YOLO model loaded successfully!\")\n",
    "                    return model\n",
    "                except ImportError:\n",
    "                    print(\"‚ö†Ô∏è ultralytics not installed. Returning file path.\")\n",
    "                    return model_path\n",
    "                    \n",
    "            else:\n",
    "                print(\"‚ùå .pt file not found in artifacts\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "loaded_model = load_yolo_model_from_mlflow(\"YOLO_Model_v2\", \"1\")\n",
    "\n",
    "if loaded_model:\n",
    "    print(\"üöÄ Model ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Managing model stages: YOLO_Model_v2 v1\n",
      "‚úÖ Model promoted to STAGING\n",
      "‚úÖ Description updated\n",
      "‚úÖ Tags added\n"
     ]
    }
   ],
   "source": [
    "def manage_model_stages(model_name, version=\"1\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(f\"üîÑ Managing model stages: {model_name} v{version}\")\n",
    "    \n",
    "    try:\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=\"Staging\",\n",
    "            archive_existing_versions=False\n",
    "        )\n",
    "        print(\"‚úÖ Model promoted to STAGING\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error promoting to staging: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            description=\"Model under test - Performance validated on validation dataset\"\n",
    "        )\n",
    "        print(\"‚úÖ Description updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating description: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\"\n",
    "        )w1sq\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"performance_tier\",\n",
    "            value=\"high\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Tags added\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding tags: {e}\")\n",
    "\n",
    "manage_model_stages(\"YOLO_Model_v2\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bf78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Usando modelo existente: YOLO_Model_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13065/4065982064.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  optimization_date = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
      "2025/11/11 01:14:07 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: YOLO_Model_v2, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Nova vers√£o registrada!\n",
      "ÔøΩÔøΩ Modelo: YOLO_Model_v2\n",
      "üî¢ Vers√£o: 2\n",
      "üîÑ date: #2025-11-11_04-14-07\n",
      "üìä mAP: 0.87\n",
      "üèÉ View run optimization_date_2025-11-11_04-14-07 at: http://localhost:5000/#/experiments/0/runs/632556e7835648b79c15a1771b2c998e\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "def register_optimization_iteration(\n",
    "    model_path, \n",
    "    base_model_name,\n",
    "    optuna_study_name,\n",
    "    best_params,\n",
    "    metrics,\n",
    "    description=\"\"\n",
    "):\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    try:\n",
    "        model = client.get_registered_model(base_model_name)\n",
    "        print(f\"üì¶ Using existing model: {base_model_name}\")\n",
    "    except:\n",
    "        client.create_registered_model(\n",
    "            name=base_model_name,\n",
    "            description=\"YOLO model with iterative optimizations via Optuna\"\n",
    "        )\n",
    "        print(f\"üÜï Base model created: {base_model_name}\")\n",
    "    \n",
    "    optimization_date = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"optimization_date_{optimization_date}\") as run:\n",
    "        \n",
    "        mlflow.log_param(\"optimization_date\", optimization_date)\n",
    "        mlflow.log_param(\"optuna_study\", optuna_study_name)\n",
    "        mlflow.log_param(\"model_type\", \"YOLO\")\n",
    "        mlflow.log_param(\"optimization_method\", \"Optuna\")\n",
    "        \n",
    "        for param_name, param_value in best_params.items():\n",
    "            mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "        mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "        \n",
    "        model_version = client.create_model_version(\n",
    "            name=base_model_name,\n",
    "            source=f\"runs:/{run.info.run_id}/model\",\n",
    "            run_id=run.info.run_id,\n",
    "            description=description or f\"Optimization #{optimization_date} - mAP: {metrics.get('mAP_50', 'N/A')}\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=base_model_name,\n",
    "            version=model_version.version,\n",
    "            key=\"optimization_date\",\n",
    "            value=str(optimization_date)\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=base_model_name,\n",
    "            version=model_version.version,\n",
    "            key=\"optuna_study\",\n",
    "            value=optuna_study_name\n",
    "        )\n",
    "        \n",
    "        print(f\"ÔøΩÔøΩ New version registered!\")\n",
    "        print(f\"üì¶ Model: {base_model_name}\")\n",
    "        print(f\"üî¢ Version: {model_version.version}\")\n",
    "        print(f\"üîÑ Date: #{optimization_date}\")\n",
    "        print(f\"üìä mAP: {metrics.get('mAP_50', 'N/A')}\")\n",
    "        \n",
    "        return model_version\n",
    "\n",
    "def register_next_optimization():\n",
    "    \n",
    "    optimization_data = {\n",
    "        \"model_path\": \"./runs/detect/best_model/weights/last.pt\",\n",
    "        \"base_model_name\": \"YOLO_Model_v2\",\n",
    "        \"optuna_study_name\": \"optuna_study_name\",\n",
    "        \"best_params\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 100,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"weight_decay\": 0.0005\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"mAP_50\": 0.87,\n",
    "            \"mAP_50_95\": 0.74,\n",
    "            \"precision\": 0.84,\n",
    "            \"recall\": 0.81,\n",
    "            \"f1_score\": 0.825\n",
    "        },\n",
    "        \"description\": \"Optimization #2 - Focus on reducing false positives\"\n",
    "    }\n",
    "    \n",
    "    return register_optimization_iteration(**optimization_data)\n",
    "\n",
    "next_version = register_next_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47210d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13065/3826863510.py:9: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  staging_versions = client.get_latest_versions(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Carregando modelo em Staging:\n",
      "üì¶ Modelo: YOLO_Model_v2 v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Arquivo do modelo: /tmp/tmpu90szjm0/model/best.pt\n",
      "üéØ Modelo YOLO carregado para infer√™ncia!\n",
      "‚úÖ Modelo v1 pronto para uso!\n"
     ]
    }
   ],
   "source": [
    "def load_staging_model_for_inference(model_name):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    staging_versions = client.get_latest_versions(\n",
    "        name=model_name,\n",
    "        stages=[\"Staging\"]\n",
    "    )\n",
    "    \n",
    "    if not staging_versions:\n",
    "        print(f\"‚ùå No Staging version for: {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    staging_version = staging_versions[0]\n",
    "    \n",
    "    print(f\"üöÄ Loading Staging model:\")\n",
    "    print(f\"üì¶ Model: {staging_version.name} v{staging_version.version}\")\n",
    "    \n",
    "    try:\n",
    "        model_uri = f\"models:/{model_name}/Staging\"\n",
    "        \n",
    "        import tempfile\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            \n",
    "            mlflow.artifacts.download_artifacts(\n",
    "                artifact_uri=staging_version.source,\n",
    "                dst_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            import glob\n",
    "            pt_files = glob.glob(f\"{temp_dir}/**/*.pt\", recursive=True)\n",
    "            \n",
    "            if pt_files:\n",
    "                model_path = pt_files[0]\n",
    "                print(f\"‚úÖ Model file: {model_path}\")\n",
    "                \n",
    "                try:\n",
    "                    from ultralytics import YOLO\n",
    "                    model = YOLO(model_path)\n",
    "                    print(f\"üéØ YOLO model loaded for inference!\")\n",
    "                    return model, staging_version\n",
    "                except ImportError:\n",
    "                    print(\"‚ö†Ô∏è ultralytics not available. Returning file path.\")\n",
    "                    return model_path, staging_version\n",
    "            else:\n",
    "                print(\"‚ùå .pt file not found\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "result = load_staging_model_for_inference(\"YOLO_Model_v2\")\n",
    "if result:\n",
    "    model, staging_version = result\n",
    "    print(f\"‚úÖ Model v{staging_version.version} ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
