{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7e3fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def get_device():\n",
    "  device = \"cpu\"\n",
    "  if (torch.cuda.is_available()):\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    device = [-1] * num_gpus\n",
    "  return device\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60bd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "models = [\"yolo12n\", \"yolo12s\"]\n",
    "data = \"./dataset/data_freeze.yaml\"\n",
    "epochs= 3\n",
    "patience= 100\n",
    "min_batch_size= 8\n",
    "max_batch_size= 8\n",
    "batch_size_step = 16\n",
    "image_size = [32, 64]\n",
    "cache = False\n",
    "optimizer = [\"SGD\", \"Adam\", \"AdamW\", \"NAdam\", \"RAdam\", \"RMSProp\"]\n",
    "multi_scale = [True, False]\n",
    "cos_lr = [True, False]\n",
    "close_mosaic_min = 1\n",
    "close_mosaic_max = 100\n",
    "amp = True\n",
    "lr0_min = 1e-5\n",
    "lr0_max = 1e-1\n",
    "momentum_min = 0.5\n",
    "momentum_max = 0.99\n",
    "weight_decay_min = 1e-6\n",
    "weight_decay_max = 1e-2\n",
    "warmup_epochs_min = 1\n",
    "warmup_epochs_max = 100\n",
    "warmup_momentum_min = 0.5\n",
    "warmup_momentum_max = 0.99\n",
    "warmup_bias_lr_min = 1e-5\n",
    "warmup_bias_lr_max = 0.2\n",
    "box_weight_min = 1e-5\n",
    "box_weight_max = 10.0\n",
    "cls_weight_min = 1e-5\n",
    "cls_weight_max = 10.0\n",
    "dfl_weight_min = 1e-5\n",
    "dfl_weight_max = 10.0\n",
    "dropout_min = 1e-5\n",
    "dropout_max = 0.5\n",
    "max_workers = max(1, (len(os.sched_getaffinity(0)) if hasattr(os, 'sched_getaffinity') else os.cpu_count()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723cc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Callable\n",
    "from collections.abc import Sequence\n",
    "import functools\n",
    "from typing import Any\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import optuna\n",
    "from optuna._experimental import experimental_class\n",
    "from optuna._experimental import experimental_func\n",
    "from optuna._imports import try_import\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from optuna.study.study import ObjectiveFuncType\n",
    "\n",
    "\n",
    "with try_import() as _imports:\n",
    "    import wandb\n",
    "\n",
    "\n",
    "@experimental_class(\"2.9.0\")\n",
    "class CustomWeightsAndBiasesCallback:\n",
    "    \"\"\"Callback to track Optuna trials with Weights & Biases.\n",
    "\n",
    "    This callback enables tracking of Optuna study in\n",
    "    Weights & Biases. The study is tracked as a single experiment\n",
    "    run, where all suggested hyperparameters and optimized metrics\n",
    "    are logged and plotted as a function of optimizer steps.\n",
    "\n",
    "    .. note::\n",
    "        User needs to be logged in to Weights & Biases before\n",
    "        using this callback in online mode. For more information, please\n",
    "        refer to `wandb setup <https://docs.wandb.ai/quickstart#1-set-up-wandb>`_.\n",
    "\n",
    "    .. note::\n",
    "        Users who want to run multiple Optuna studies within the same process\n",
    "        should call ``wandb.finish()`` between subsequent calls to\n",
    "        :meth:`optuna.study.Study.optimize`. Calling ``wandb.finish()`` is not necessary\n",
    "        if you are running one Optuna study per process.\n",
    "\n",
    "    .. note::\n",
    "        To ensure correct trial order in Weights & Biases, this callback\n",
    "        should only be used with ``study.optimize(n_jobs=1)``.\n",
    "\n",
    "\n",
    "    Example:\n",
    "\n",
    "        Add Weights & Biases callback to Optuna optimization.\n",
    "\n",
    "        .. code::\n",
    "\n",
    "            import optuna\n",
    "            from optuna_integration.wandb import WeightsAndBiasesCallback\n",
    "\n",
    "\n",
    "            def objective(trial):\n",
    "                x = trial.suggest_float(\"x\", -10, 10)\n",
    "                return (x - 2) ** 2\n",
    "\n",
    "\n",
    "            study = optuna.create_study()\n",
    "\n",
    "            wandb_kwargs = {\"project\": \"my-project\"}\n",
    "            wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)\n",
    "\n",
    "            study.optimize(objective, n_trials=10, callbacks=[wandbc])\n",
    "\n",
    "\n",
    "\n",
    "        Weights & Biases logging in multirun mode.\n",
    "\n",
    "        .. code::\n",
    "\n",
    "            import optuna\n",
    "            from optuna_integration.wandb import WeightsAndBiasesCallback\n",
    "\n",
    "            wandb_kwargs = {\"project\": \"my-project\"}\n",
    "            wandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n",
    "\n",
    "\n",
    "            @wandbc.track_in_wandb()\n",
    "            def objective(trial):\n",
    "                x = trial.suggest_float(\"x\", -10, 10)\n",
    "                return (x - 2) ** 2\n",
    "\n",
    "\n",
    "            study = optuna.create_study()\n",
    "            study.optimize(objective, n_trials=10, callbacks=[wandbc])\n",
    "\n",
    "\n",
    "    Args:\n",
    "        metric_name:\n",
    "            Name assigned to optimized metric. In case of multi-objective optimization,\n",
    "            list of names can be passed. Those names will be assigned\n",
    "            to metrics in the order returned by objective function.\n",
    "            If single name is provided, or this argument is left to default value,\n",
    "            it will be broadcasted to each objective with a number suffix in order\n",
    "            returned by objective function e.g. two objectives and default metric name\n",
    "            will be logged as ``value_0`` and ``value_1``. The number of metrics must be\n",
    "            the same as the number of values objective function returns.\n",
    "        wandb_kwargs:\n",
    "            Set of arguments passed when initializing Weights & Biases run.\n",
    "            Please refer to `Weights & Biases API documentation\n",
    "            <https://docs.wandb.ai/ref/python/init>`_ for more details.\n",
    "        as_multirun:\n",
    "            Creates new runs for each trial. Useful for generating W&B Sweeps like\n",
    "            panels (for ex., parameter importance, parallel coordinates, etc).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric_name: str | Sequence[str] = \"value\",\n",
    "        wandb_kwargs: dict[str, Any] | None = None,\n",
    "        as_multirun: bool = False,\n",
    "    ) -> None:\n",
    "        _imports.check()\n",
    "\n",
    "        if not isinstance(metric_name, Sequence):\n",
    "            raise TypeError(\n",
    "                \"Expected metric_name to be string or sequence of strings, got {}.\".format(\n",
    "                    type(metric_name)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self._metric_name = metric_name\n",
    "        self._wandb_kwargs = wandb_kwargs or {}\n",
    "        self._as_multirun = as_multirun\n",
    "\n",
    "        if not self._as_multirun:\n",
    "            self._initialize_run()\n",
    "\n",
    "    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
    "        # Failed and pruned trials have `None` as values.\n",
    "        metrics = {}\n",
    "        values: list = trial.values\n",
    "        run = wandb.run\n",
    "\n",
    "        if values is not None:\n",
    "            if isinstance(self._metric_name, str):\n",
    "                if len(values) > 1:\n",
    "                    # Broadcast default name for multi-objective optimization.\n",
    "                    names = [\"{}_{}\".format(self._metric_name, i) for i in range(len(values))]\n",
    "\n",
    "                else:\n",
    "                    names = [self._metric_name]\n",
    "\n",
    "            else:\n",
    "                if len(self._metric_name) != len(values):\n",
    "                    raise ValueError(\n",
    "                        \"Running multi-objective optimization \"\n",
    "                        \"with {} objective values, but {} names specified. \"\n",
    "                        \"Match objective values and names, or use default broadcasting.\".format(\n",
    "                            len(values), len(self._metric_name)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    names = [*self._metric_name]\n",
    "\n",
    "            metrics = {name: value for name, value in zip(names, values)}   \n",
    "\n",
    "        if self._as_multirun:\n",
    "            metrics[\"trial_number\"] = trial.number\n",
    "\n",
    "        attributes = {\"direction\": [d.name for d in study.directions]}\n",
    "\n",
    "        step = trial.number if wandb.run else None\n",
    "\n",
    "        # Might create extra runs if a user logs in wandb but doesn't use the decorator.\n",
    "        \n",
    "        if not run:\n",
    "            run = self._initialize_run()\n",
    "            run.name = f\"trial/{trial.number}\"\n",
    "\n",
    "        run.log({**trial.params, **metrics}, step=step)\n",
    "\n",
    "        if self._as_multirun:\n",
    "            run.config.update({**attributes, **trial.params})  # type: ignore[no-untyped-call]\n",
    "            run.tags = tuple(self._wandb_kwargs.get(\"tags\", ())) + (study.study_name,)\n",
    "\n",
    "            if trial.state == TrialState.COMPLETE:\n",
    "                run.log({\n",
    "                    \"status\": \"completed\",\n",
    "                    \"trial_state\": \"COMPLETE\"\n",
    "                    })\n",
    "                run.finish()\n",
    "                \n",
    "            elif trial.state == TrialState.PRUNED:\n",
    "                run.log({\n",
    "                    \"status\": \"pruned\",\n",
    "                    \"trial_state\": \"PRUNED\"\n",
    "                })  \n",
    "                run.mark_preempting()\n",
    "            elif trial.state == TrialState.FAIL:\n",
    "                run.log({\n",
    "                    \"status\": \"failed\",\n",
    "                    \"trial_state\": \"FAIL\",\n",
    "                    \"error\": \"Trial failed during execution\"\n",
    "                    })\n",
    "                run.mark_preempting() \n",
    "            \n",
    "        else:\n",
    "            run.config.update(attributes)  # type: ignore[no-untyped-call]\n",
    "    \n",
    "\n",
    "    def _initialize_run(self) -> \"wandb.sdk.wandb_run.Run\":\n",
    "        \"\"\"Initializes Weights & Biases run.\"\"\"\n",
    "        run = wandb.init(**self._wandb_kwargs)\n",
    "        if not isinstance(run, wandb.sdk.wandb_run.Run):\n",
    "            raise RuntimeError(\n",
    "                \"Cannot create a Run. \"\n",
    "                \"Expected wandb.sdk.wandb_run.Run as a return. \"\n",
    "                f\"Got: {type(run)}.\"\n",
    "            )\n",
    "        return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4236ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import datetime as dt\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "\n",
    "def train(trial):\n",
    "    # Configurar nomes e groups\n",
    "    today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    group_name = f\"study-{today}__build_{os.getenv('GIT_SHA','local')}__data_v42\"\n",
    "    run_name = f\"trial/{trial.number}\"\n",
    "    \n",
    "    # Inicializar wandb no INÃCIO da trial\n",
    "    run = wandb.init(\n",
    "        project=\"yolo-optimization-monitor\",\n",
    "        entity=\"levybessa-puc\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "        tags=[\"YOLO\", \"optuna\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    wandb.log({\n",
    "        \"status\": \"running\",\n",
    "        \"trial_number\": trial.number,\n",
    "        \"stage\": \"initializing\",\n",
    "    })\n",
    "    \n",
    "    print(f\"ðŸš€ Started wandb run: {run_name} (ID: {run.id})\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Model selection\n",
    "        model_name = trial.suggest_categorical(\"model\", models)\n",
    "        \n",
    "        # Basic training parameters\n",
    "        batch_size = trial.suggest_int(\"batch_size\", min_batch_size, max_batch_size, step=batch_size_step)\n",
    "        imgsz = trial.suggest_categorical(\"imgsz\", image_size)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", optimizer)\n",
    "        \n",
    "        # Training settings\n",
    "        multi_scale_enabled = trial.suggest_categorical(\"multi_scale\", multi_scale)\n",
    "        cos_lr_enabled = trial.suggest_categorical(\"cos_lr\", cos_lr)\n",
    "        close_mosaic = trial.suggest_int(\"close_mosaic\", close_mosaic_min, close_mosaic_max)\n",
    "        \n",
    "        # Learning rate parameters\n",
    "        lr0 = trial.suggest_float(\"lr0\", lr0_min, lr0_max, log=True)\n",
    "        momentum = trial.suggest_float(\"momentum\", momentum_min, momentum_max)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", weight_decay_min, weight_decay_max, log=True)\n",
    "        \n",
    "        # Warm-up parameters\n",
    "        warmup_epochs = trial.suggest_int(\"warmup_epochs\", warmup_epochs_min, warmup_epochs_max)\n",
    "        warmup_momentum = trial.suggest_float(\"warmup_momentum\", warmup_momentum_min, warmup_momentum_max)\n",
    "        warmup_bias_lr = trial.suggest_float(\"warmup_bias_lr\", warmup_bias_lr_min, warmup_bias_lr_max)\n",
    "        \n",
    "        # Loss function weights\n",
    "        box_weight = trial.suggest_float(\"box_weight\", box_weight_min, box_weight_max)\n",
    "        cls_weight = trial.suggest_float(\"cls_weight\", cls_weight_min, cls_weight_max)\n",
    "        dfl_weight = trial.suggest_float(\"dfl_weight\", dfl_weight_min, dfl_weight_max)\n",
    "        \n",
    "        # Dropout\n",
    "        dropout = trial.suggest_float(\"dropout\", dropout_min, dropout_max)\n",
    "        \n",
    "        # ===== CONFIGURAR WANDB COM HIPERPARÃ‚METROS =====\n",
    "        hyperparams = {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"imgsz\": imgsz,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"multi_scale\": multi_scale_enabled,\n",
    "            \"cos_lr\": cos_lr_enabled,\n",
    "            \"close_mosaic\": close_mosaic,\n",
    "            \"lr0\": lr0,\n",
    "            \"momentum\": momentum,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"warmup_epochs\": warmup_epochs,\n",
    "            \"warmup_momentum\": warmup_momentum,\n",
    "            \"warmup_bias_lr\": warmup_bias_lr,\n",
    "            \"box_weight\": box_weight,\n",
    "            \"cls_weight\": cls_weight,\n",
    "            \"dfl_weight\": dfl_weight,\n",
    "            \"dropout\": dropout,\n",
    "            # ConfiguraÃ§Ãµes fixas\n",
    "            \"epochs\": epochs,\n",
    "            \"patience\": patience,\n",
    "            \"cache\": cache,\n",
    "            \"amp\": amp,\n",
    "            \"data\": data,\n",
    "            \"max_workers\": max_workers\n",
    "        }\n",
    "        \n",
    "        wandb.config.update(hyperparams)\n",
    "        \n",
    "        print(f\"ðŸ“‹ Trial {trial.number} parameters:\")\n",
    "        print(f\"   Model: {model_name}, Batch: {batch_size}, ImgSz: {imgsz}\")\n",
    "        print(f\"   Optimizer: {optimizer_name}, LR: {lr0:.6f}, Momentum: {momentum:.3f}\")\n",
    "        print(f\"   Group: {group_name}\")\n",
    "        print(f\"   Run: {run_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        \n",
    "        model = YOLO(f\"{model_name}.pt\")\n",
    "        \n",
    "\n",
    "        print(f\"ðŸ‹ï¸ Starting training for trial {trial.number}...\")\n",
    "        \n",
    "        # IMPORTANTE: Usar wandb ativo (project=None)\n",
    "        results = model.train(\n",
    "            project=None,  # â† Usar wandb run ativa\n",
    "            data=data,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            batch=batch_size,\n",
    "            imgsz=imgsz,\n",
    "            cache=cache,\n",
    "            optimizer=optimizer_name,\n",
    "            multi_scale=multi_scale_enabled,\n",
    "            cos_lr=cos_lr_enabled,\n",
    "            close_mosaic=close_mosaic,\n",
    "            amp=amp,\n",
    "            lr0=lr0,\n",
    "            momentum=momentum,\n",
    "            weight_decay=weight_decay,\n",
    "            warmup_epochs=warmup_epochs,\n",
    "            warmup_momentum=warmup_momentum,\n",
    "            warmup_bias_lr=warmup_bias_lr,\n",
    "            box=box_weight,\n",
    "            cls=cls_weight,\n",
    "            dfl=dfl_weight,\n",
    "            dropout=dropout,\n",
    "            verbose=False,\n",
    "            save=False,\n",
    "            plots=True,  # â† Gerar plots no wandb\n",
    "            device=get_device(),\n",
    "            workers=max_workers\n",
    "        )\n",
    "        \n",
    "        # ===== RESULTADOS FINAIS =====\n",
    "        mean_precision, mean_recall, mAP50, mAP50_90 = tuple(results.box.mean_results())\n",
    "        \n",
    "        # Coletar mÃ©tricas adicionais se disponÃ­veis\n",
    "        final_metrics = {\n",
    "            \"mAP50_95\": mAP50_90,\n",
    "            \"mean precision\": mean_precision,\n",
    "            \"mean recall\": mean_recall,\n",
    "            \"mAP50\": mAP50,\n",
    "            \"status\": \"completed\",\n",
    "            \"stage\": \"finished\",\n",
    "            \"trial_number\": trial.number,\n",
    "        }\n",
    "        \n",
    "            \n",
    "        wandb.log(final_metrics)\n",
    "        wandb.finish()\n",
    "        print(f\"âœ… Trial {trial.number} completed successfully: mAP = {mAP50_90:.4f}\")\n",
    "        print(f\"ðŸ”— Run URL: {run.url}\")\n",
    "        \n",
    "        return mAP50\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        # ===== ERRO CUDA OUT OF MEMORY =====\n",
    "        error_info = {\n",
    "            \"status\": \"failed\",\n",
    "            \"error_type\": \"CUDA_OOM\", \n",
    "            \"error_message\": str(e),\n",
    "            \"trial_number\": trial.number,\n",
    "            \"stage\": \"cuda_oom_error\",\n",
    "        }\n",
    "        \n",
    "        wandb.log(error_info)\n",
    "        \n",
    "        # Marcar como crashed\n",
    "        try:\n",
    "            run.mark_preempting()\n",
    "        except Exception as mark_error:\n",
    "            print(f\"âš ï¸ Could not mark as preempting: {mark_error}\")\n",
    "        \n",
    "        print(f\"ðŸ’¥ Trial {trial.number} - CUDA Out of Memory Error!\")\n",
    "        print(f\"   Model: {model_name}, Batch: {batch_size}, ImgSz: {imgsz}\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        print(f\"   Device: {get_device()}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        # ===== ERRO GERAL =====\n",
    "        error_info = {\n",
    "            \"status\": \"failed\",\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"error_message\": str(e),\n",
    "            \"trial_number\": trial.number,\n",
    "            \"stage\": \"general_error\",\n",
    "        }\n",
    "        \n",
    "        # Adicionar contexto se variÃ¡veis existirem\n",
    "        wandb.log(error_info)\n",
    "        \n",
    "        # Marcar como Failed\n",
    "        try:\n",
    "            wandb.finish(exit_code=1) \n",
    "        except Exception as mark_error:\n",
    "            print(f\"âš ï¸ Could not mark as preempting: {mark_error}\")\n",
    "        \n",
    "        print(f\"âŒ Trial {trial.number} - Error: {type(e).__name__}\")\n",
    "        print(f\"   Details: {str(e)}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # ===== CLEANUP E FINALIZAÃ‡ÃƒO =====\n",
    "        if 'model' in locals():\n",
    "            try:\n",
    "                del model\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Cleanup failed: {e}\")\n",
    "        \n",
    "        print(f\"ðŸ§¹ Trial {trial.number} cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cae19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import BasePruner\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "class ErrorPruner(BasePruner):\n",
    "    def __init__(self, max_consecutive_errors=3):\n",
    "        self.max_consecutive_errors = max_consecutive_errors\n",
    "        self.error_count = 0\n",
    "\n",
    "    def prune(self, study, trial):\n",
    "        if trial.state == TrialState.FAIL:\n",
    "            self.error_count += 1\n",
    "            print(f\"Trial {trial.number} failed. Consecutive errors: {self.error_count}\")\n",
    "            if self.error_count >= self.max_consecutive_errors:\n",
    "                print(f\"Many consecutive errors ({self.error_count}). Consider reviewing configuration.\")\n",
    "            return True\n",
    "        else:\n",
    "            self.error_count = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778c74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(n_trials):\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=\"YOLO_Hyperparameter_Optimization\",\n",
    "                                storage=\"sqlite:///yolo_hyperparameter_optimization.db\",\n",
    "                                load_if_exists=False,\n",
    "                                pruner=ErrorPruner(max_consecutive_errors=5)\n",
    "                                )\n",
    "    study.optimize(train, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best hyperparameters found:\")\n",
    "    print(\"=\" * 50)\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest mAP50-95: {study.best_value:.4f}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a44fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00bb07af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:16:42,270] A new study created in RDB with name: YOLO_Hyperparameter_Optimization\n",
      "/tmp/ipykernel_10560/2727927038.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Started wandb run: trial/0 (ID: o133djob)\n",
      "ðŸ“‹ Trial 0 parameters:\n",
      "   Model: yolo12s, Batch: 8, ImgSz: 64\n",
      "   Optimizer: RAdam, LR: 0.000639, Momentum: 0.825\n",
      "   Group: study-2025-11-11__build_local__data_v42\n",
      "   Run: trial/0\n",
      "----------------------------------------------------------------------\n",
      "ðŸ‹ï¸ Starting training for trial 0...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=2.2914060461839503, cache=False, cfg=None, classes=None, close_mosaic=28, cls=3.975330460510305, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=7.490904157257988, dnn=False, dropout=0.06239473164324965, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0006385338272821792, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.8251297608770989, mosaic=1.0, multi_scale=True, name=train37, nbs=64, nms=False, opset=None, optimize=False, optimizer=RAdam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train37, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.10440258444025446, warmup_epochs=15, warmup_momentum=0.8754730086501724, weight_decay=0.00393108110693011, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
      " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 21        [14, 17, 20]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "YOLOv12s summary: 272 layers, 9,254,297 parameters, 9,254,281 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1836.3Â±713.1 MB/s, size: 29.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 599/599 859.8Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1189.3Â±516.2 MB/s, size: 32.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 86/86 86.9Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train37/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RAdam(lr=0.0006385338272821792, momentum=0.8251297608770989) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.00393108110693011), 119 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train37\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3     0.293G      1.042      26.27      8.297         10         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 7.0it/s 10.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 9.3it/s 0.6s0.1s\n",
      "                   all         86        103     0.0214     0.0959       0.02    0.00656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3     0.312G     0.9429      21.79       7.52         13         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 6.6it/s 11.4s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 11.6it/s 0.5s.1s\n",
      "                   all         86        103     0.0327      0.149     0.0241     0.0083\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3     0.314G     0.9507      20.53      7.106         16         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 8.8it/s 8.5s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 11.6it/s 0.5s.1s\n",
      "                   all         86        103     0.0951      0.249      0.032     0.0107\n",
      "\n",
      "3 epochs completed in 0.009 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train37/weights/last.pt, 18.9MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train37/weights/best.pt, 18.9MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/train37/weights/best.pt...\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12s summary (fused): 159 layers, 9,232,041 parameters, 0 gradients, 21.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 6.4it/s 0.9s0.2s\n",
      "                   all         86        103     0.0923      0.249     0.0322     0.0107\n",
      "Speed: 0.3ms preprocess, 4.5ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train37\u001b[0m\n",
      "âœ… Trial 0 completed successfully: mAP = 0.0107\n",
      "ðŸ”— Run URL: https://wandb.ai/levybessa-puc/yolo-optimization-monitor/runs/o133djob\n",
      "ðŸ§¹ Trial 0 cleanup completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:17:26,994] Trial 0 finished with value: 0.032170174264621994 and parameters: {'model': 'yolo12s', 'batch_size': 8, 'imgsz': 64, 'optimizer': 'RAdam', 'multi_scale': True, 'cos_lr': True, 'close_mosaic': 28, 'lr0': 0.0006385338272821792, 'momentum': 0.8251297608770989, 'weight_decay': 0.00393108110693011, 'warmup_epochs': 15, 'warmup_momentum': 0.8754730086501724, 'warmup_bias_lr': 0.10440258444025446, 'box_weight': 2.2914060461839503, 'cls_weight': 3.975330460510305, 'dfl_weight': 7.490904157257988, 'dropout': 0.06239473164324965}. Best is trial 0 with value: 0.032170174264621994.\n",
      "/tmp/ipykernel_10560/2727927038.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Started wandb run: trial/1 (ID: ao2wu95t)\n",
      "ðŸ“‹ Trial 1 parameters:\n",
      "   Model: yolo12s, Batch: 8, ImgSz: 64\n",
      "   Optimizer: Adam, LR: 0.001862, Momentum: 0.566\n",
      "   Group: study-2025-11-11__build_local__data_v42\n",
      "   Run: trial/1\n",
      "----------------------------------------------------------------------\n",
      "ðŸ‹ï¸ Starting training for trial 1...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=0.6606002562088968, cache=False, cfg=None, classes=None, close_mosaic=28, cls=5.479790741726319, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=5.695735860690563, dnn=False, dropout=0.3558863694733579, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0018622064907595505, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.5659799667499514, mosaic=1.0, multi_scale=True, name=train38, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train38, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1873595898143793, warmup_epochs=44, warmup_momentum=0.9273554582263195, weight_decay=0.00019280861790687195, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
      " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 21        [14, 17, 20]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "YOLOv12s summary: 272 layers, 9,254,297 parameters, 9,254,281 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1768.9Â±704.1 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 599/599 1.2Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 396.4Â±67.9 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 86/86 92.9Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train38/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.0018622064907595505, momentum=0.5659799667499514) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.00019280861790687195), 119 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train38\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3     0.295G     0.3069      33.61      6.098         10         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 6.1it/s 12.2s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 11.1it/s 0.5s.3s\n",
      "                   all         86        103      0.425      0.131     0.0592     0.0185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3     0.295G     0.2786      29.99      5.415         13         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 7.8it/s 9.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 12.2it/s 0.5s.3s\n",
      "                   all         86        103      0.361     0.0569     0.0186    0.00468\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3     0.295G     0.2776      28.45      5.215         16         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 7.4it/s 10.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 11.7it/s 0.5s.4s\n",
      "                   all         86        103      0.134      0.246     0.0961     0.0306\n",
      "\n",
      "3 epochs completed in 0.010 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train38/weights/last.pt, 18.9MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train38/weights/best.pt, 18.9MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/train38/weights/best.pt...\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12s summary (fused): 159 layers, 9,232,041 parameters, 0 gradients, 21.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 8.2it/s 0.7s0.2s\n",
      "                   all         86        103      0.134      0.246     0.0965     0.0307\n",
      "Speed: 0.3ms preprocess, 2.9ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train38\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:18:10,042] Trial 1 finished with value: 0.09647233467844067 and parameters: {'model': 'yolo12s', 'batch_size': 8, 'imgsz': 64, 'optimizer': 'Adam', 'multi_scale': True, 'cos_lr': False, 'close_mosaic': 28, 'lr0': 0.0018622064907595505, 'momentum': 0.5659799667499514, 'weight_decay': 0.00019280861790687195, 'warmup_epochs': 44, 'warmup_momentum': 0.9273554582263195, 'warmup_bias_lr': 0.1873595898143793, 'box_weight': 0.6606002562088968, 'cls_weight': 5.479790741726319, 'dfl_weight': 5.695735860690563, 'dropout': 0.3558863694733579}. Best is trial 1 with value: 0.09647233467844067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trial 1 completed successfully: mAP = 0.0307\n",
      "ðŸ”— Run URL: https://wandb.ai/levybessa-puc/yolo-optimization-monitor/runs/ao2wu95t\n",
      "ðŸ§¹ Trial 1 cleanup completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10560/2727927038.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = dt.datetime.utcnow().strftime(\"%Y-%m-%d\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Started wandb run: trial/2 (ID: tpo0ucjp)\n",
      "ðŸ“‹ Trial 2 parameters:\n",
      "   Model: yolo12s, Batch: 8, ImgSz: 64\n",
      "   Optimizer: RMSProp, LR: 0.000023, Momentum: 0.678\n",
      "   Group: study-2025-11-11__build_local__data_v42\n",
      "   Run: trial/2\n",
      "----------------------------------------------------------------------\n",
      "ðŸ‹ï¸ Starting training for trial 2...\n",
      "New https://pypi.org/project/ultralytics/8.3.227 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Searching for 1 idle GPUs with free memory >= 20.0% and free utilization >= 0.0%...\n",
      "Selected idle CUDA devices [0]\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=1.015902569260478, cache=False, cfg=None, classes=None, close_mosaic=22, cls=1.3689886901106696, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=8.790430761439183, dnn=False, dropout=0.23528646906133144, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=2.2934869573592512e-05, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.6780591507547779, mosaic=1.0, multi_scale=True, name=train39, nbs=64, nms=False, opset=None, optimize=False, optimizer=RMSProp, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/train39, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.049705711963199686, warmup_epochs=26, warmup_momentum=0.774125908088876, weight_decay=6.746734765069223e-05, workers=11, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
      " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 21        [14, 17, 20]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "YOLOv12s summary: 272 layers, 9,254,297 parameters, 9,254,281 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1898.6Â±811.3 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 599/599 876.6Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.6Â±1.4 ms, read: 269.4Â±103.8 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 86/86 34.5Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/train39/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m RMSprop(lr=2.2934869573592512e-05, momentum=0.6780591507547779) with parameter groups 113 weight(decay=0.0), 120 weight(decay=6.746734765069223e-05), 119 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 11 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train39\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3     0.469G     0.5344      9.486      9.993         10         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 8.1it/s 9.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 10.7it/s 0.6s.3s\n",
      "                   all         86        103      0.667      0.124    0.00076   0.000185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3     0.469G     0.4988      8.539        8.6         13         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 7.5it/s 10.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 12.0it/s 0.5s.1s\n",
      "                   all         86        103    0.00163       0.36    0.00294   0.000473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3     0.469G     0.4701      7.999      8.075         16         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 8.9it/s 8.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 1.6it/s 3.8s0.3ss\n",
      "                   all         86        103    0.00172      0.404    0.00776    0.00155\n",
      "\n",
      "3 epochs completed in 0.009 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train39/weights/last.pt, 18.9MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/train39/weights/best.pt, 18.9MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/train39/weights/best.pt...\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12s summary (fused): 159 layers, 9,232,041 parameters, 0 gradients, 21.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 8.3it/s 0.7s0.2s\n",
      "                   all         86        103    0.00172      0.404    0.00783    0.00155\n",
      "Speed: 0.2ms preprocess, 3.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/train39\u001b[0m\n",
      "âœ… Trial 2 completed successfully: mAP = 0.0016\n",
      "ðŸ”— Run URL: https://wandb.ai/levybessa-puc/yolo-optimization-monitor/runs/tpo0ucjp\n",
      "ðŸ§¹ Trial 2 cleanup completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:18:54,987] Trial 2 finished with value: 0.007833426090043899 and parameters: {'model': 'yolo12s', 'batch_size': 8, 'imgsz': 64, 'optimizer': 'RMSProp', 'multi_scale': True, 'cos_lr': True, 'close_mosaic': 22, 'lr0': 2.2934869573592512e-05, 'momentum': 0.6780591507547779, 'weight_decay': 6.746734765069223e-05, 'warmup_epochs': 26, 'warmup_momentum': 0.774125908088876, 'warmup_bias_lr': 0.049705711963199686, 'box_weight': 1.015902569260478, 'cls_weight': 1.3689886901106696, 'dfl_weight': 8.790430761439183, 'dropout': 0.23528646906133144}. Best is trial 1 with value: 0.09647233467844067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:\n",
      "==================================================\n",
      "model: yolo12s\n",
      "batch_size: 8\n",
      "imgsz: 64\n",
      "optimizer: Adam\n",
      "multi_scale: True\n",
      "cos_lr: False\n",
      "close_mosaic: 28\n",
      "lr0: 0.0018622064907595505\n",
      "momentum: 0.5659799667499514\n",
      "weight_decay: 0.00019280861790687195\n",
      "warmup_epochs: 44\n",
      "warmup_momentum: 0.9273554582263195\n",
      "warmup_bias_lr: 0.1873595898143793\n",
      "box_weight: 0.6606002562088968\n",
      "cls_weight: 5.479790741726319\n",
      "dfl_weight: 5.695735860690563\n",
      "dropout: 0.3558863694733579\n",
      "\n",
      "Best mAP50-95: 0.0965\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "study = optimize_hyperparameters(n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfab5c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.227 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=0.6606002562088968, cache=False, cfg=None, classes=None, close_mosaic=28, cls=5.479790741726319, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./dataset/data_freeze.yaml, degrees=0.0, deterministic=True, device=0, dfl=5.695735860690563, dnn=False, dropout=0.3558863694733579, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=64, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0018622064907595505, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.5659799667499514, mosaic=1.0, multi_scale=True, name=best_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1873595898143793, warmup_epochs=44, warmup_momentum=0.9273554582263195, weight_decay=0.00019280861790687195, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
      "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
      " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
      " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
      " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 21        [14, 17, 20]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "YOLOv12s summary: 272 layers, 9,254,297 parameters, 9,254,281 gradients, 21.5 GFLOPs\n",
      "\n",
      "Transferred 685/691 items from pretrained weights\n",
      "Freezing layer 'model.21.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1720.7Â±816.4 MB/s, size: 29.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/train/labels.cache... 599 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 599/599 1.5Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 332.9Â±53.2 MB/s, size: 39.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/matheus/Documents/PFP-IA-Research/dataset/valid/labels.cache... 86 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 86/86 85.0Kit/s 0.0s\n",
      "Plotting labels to /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.0018622064907595505, momentum=0.5659799667499514) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.00019280861790687195), 119 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3      0.32G     0.2994      33.25      6.034         15         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 8.0it/s 9.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 12.8it/s 0.5s.3s\n",
      "                   all         86        103   0.000926      0.238    0.00243    0.00039\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3     0.324G     0.2883      30.84      5.422          9         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 7.8it/s 9.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 13.0it/s 0.5s.3s\n",
      "                   all         86        103      0.668     0.0543    0.00125   0.000253\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3     0.324G     0.2708      28.68      5.059         18         96: 100% â”â”â”â”â”â”â”â”â”â”â”â” 75/75 6.1it/s 12.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 12.3it/s 0.5s.3s\n",
      "                   all         86        103      0.743      0.101     0.0791     0.0216\n",
      "\n",
      "3 epochs completed in 0.010 hours.\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/last.pt, 18.9MB\n",
      "Optimizer stripped from /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/best.pt, 18.9MB\n",
      "\n",
      "Validating /home/matheus/Documents/PFP-IA-Research/runs/detect/best_model/weights/best.pt...\n",
      "Ultralytics 8.3.224 ðŸš€ Python-3.12.8 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "YOLOv12s summary (fused): 159 layers, 9,232,041 parameters, 0 gradients, 21.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 9.0it/s 0.7s0.4s\n",
      "                   all         86        103      0.744      0.101     0.0799      0.022\n",
      "                 Paper         29         29          1          0     0.0474     0.0124\n",
      "                  Rock         37         43      0.232      0.302      0.144     0.0387\n",
      "              Scissors         26         31          1          0     0.0479     0.0149\n",
      "Speed: 0.3ms preprocess, 2.5ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/matheus/Documents/PFP-IA-Research/runs/detect/best_model\u001b[0m\n",
      "Optimization completed!\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "final_model = YOLO(f\"{best_params['model']}.pt\")\n",
    "\n",
    "final_results = final_model.train(\n",
    "    data=data,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    batch=best_params['batch_size'],\n",
    "    imgsz=best_params['imgsz'],\n",
    "    cache=cache,\n",
    "    optimizer=best_params['optimizer'],\n",
    "    multi_scale=best_params['multi_scale'],\n",
    "    cos_lr=best_params['cos_lr'],\n",
    "    close_mosaic=best_params['close_mosaic'],\n",
    "    amp=amp,\n",
    "    lr0=best_params['lr0'],\n",
    "    momentum=best_params['momentum'],\n",
    "    weight_decay=best_params['weight_decay'],\n",
    "    warmup_epochs=best_params['warmup_epochs'],\n",
    "    warmup_momentum=best_params['warmup_momentum'],\n",
    "    warmup_bias_lr=best_params['warmup_bias_lr'],\n",
    "    box=best_params['box_weight'],\n",
    "    cls=best_params['cls_weight'],\n",
    "    dfl=best_params['dfl_weight'],\n",
    "    dropout=best_params['dropout'],\n",
    "    name='best_model',\n",
    "    save=True,\n",
    "    plots=True\n",
    ")\n",
    "\n",
    "print(\"Optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e12096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ†• Modelo 'YOLO_Model_v2' criado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/11 00:54:22 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: YOLO_Model_v2, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Upload concluÃ­do! Run ID: 443a15e627044448a0429069dbd6f562\n",
      "ðŸƒ View run YOLO_Upload_v2 at: http://localhost:5000/#/experiments/0/runs/443a15e627044448a0429069dbd6f562\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/0\n",
      "ðŸŽ¯ Modelo registrado!\n",
      "ðŸ“¦ Nome: YOLO_Model_v2\n",
      "ðŸ”¢ VersÃ£o: 1\n",
      "ðŸŒ Acesse: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://localhost:9444'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAIOSFODNN7EXAMPLE'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "model_path = \"./runs/detect/best_model/weights/best.pt\"\n",
    "model_name = \"YOLO_Model_v2\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"âŒ File not found: {model_path}\")\n",
    "    exit(1)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(\n",
    "        name=model_name,\n",
    "        description=\"YOLO model optimized with Optuna\"\n",
    "    )\n",
    "    print(f\"ðŸ†• Model '{model_name}' created\")\n",
    "except Exception as e:\n",
    "    print(f\"ðŸ“‹ Model already exists: {e}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"YOLO_Upload_v2\") as run:\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"YOLO\")\n",
    "    mlflow.log_param(\"optimization\", \"Optuna\")\n",
    "    \n",
    "    mlflow.log_metric(\"mAP_50\", 0.85)\n",
    "    mlflow.log_metric(\"precision\", 0.82)\n",
    "    \n",
    "    mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"âœ… Upload completed! Run ID: {run_id}\")\n",
    "\n",
    "try:\n",
    "    model_version = client.create_model_version(\n",
    "        name=model_name,\n",
    "        source=f\"runs:/{run_id}/model\",\n",
    "        run_id=run_id,\n",
    "        description=\"Optimized version with Optuna (mAP: 0.85)\"\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Model registered!\")\n",
    "    print(f\"ðŸ“¦ Name: {model_version.name}\")\n",
    "    print(f\"ðŸ”¢ Version: {model_version.version}\")\n",
    "    print(f\"ðŸŒ Access: http://localhost:5000\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Registration error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e851da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Modelo: YOLO_Model_v2\n",
      "ðŸ“ DescriÃ§Ã£o: Modelo YOLO otimizado com Optuna\n",
      "ðŸ“… Criado em: 1762833262051\n",
      "ðŸ”„ Ãšltima modificaÃ§Ã£o: 1762833262323\n",
      "\n",
      "ï¿½ï¿½ VersÃµes disponÃ­veis: 1\n",
      "==================================================\n",
      "ðŸ“‹ VersÃ£o: 1\n",
      "ðŸ“ Status: None\n",
      "ðŸ“ DescriÃ§Ã£o: VersÃ£o otimizada com Optuna (mAP: 0.85)\n",
      "ðŸ”— Run ID: 443a15e627044448a0429069dbd6f562\n",
      "ðŸ“ Source: runs:/443a15e627044448a0429069dbd6f562/model\n",
      "ðŸ“… Criado: 1762833262323\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "def explore_registered_model(model_name):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    model = client.get_registered_model(model_name)\n",
    "    \n",
    "    print(f\"ðŸ“¦ Model: {model.name}\")\n",
    "    print(f\"ðŸ“ Description: {model.description}\")\n",
    "    print(f\"ðŸ“… Created at: {model.creation_timestamp}\")\n",
    "    print(f\"ðŸ”„ Last updated: {model.last_updated_timestamp}\")\n",
    "    \n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¢ Available versions: {len(versions)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for version in versions:\n",
    "        print(f\"ðŸ“‹ Version: {version.version}\")\n",
    "        print(f\"ðŸ“ Status: {version.current_stage}\")\n",
    "        print(f\"ðŸ“ Description: {version.description}\")\n",
    "        print(f\"ðŸ”— Run ID: {version.run_id}\")\n",
    "        print(f\"ï¿½ï¿½ Source: {version.source}\")\n",
    "        print(f\"ï¿½ï¿½ Created: {version.creation_timestamp}\")\n",
    "        \n",
    "        if hasattr(version, 'tags') and version.tags:\n",
    "            print(f\"ðŸ·ï¸ Tags: {version.tags}\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return model, versions\n",
    "\n",
    "model, versions = explore_registered_model(\"YOLO_Model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150723bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda3/envs/openmmlab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Baixando modelo: YOLO_Model_v2 v1\n",
      "ðŸ“ Source: runs:/443a15e627044448a0429069dbd6f562/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 50.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo baixado em: ./downloaded_model\n",
      "ðŸ“„ ./downloaded_model/model/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_registered_model(model_name, version=\"1\", download_path=\"./downloaded_model\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    try:\n",
    "        model_version = client.get_model_version(model_name, version)\n",
    "        \n",
    "        print(f\"ðŸ“¥ Downloading model: {model_name} v{version}\")\n",
    "        print(f\"ðŸ“ Source: {model_version.source}\")\n",
    "        \n",
    "        model_uri = f\"models:/{model_name}/{version}\"\n",
    "        \n",
    "        import mlflow.artifacts\n",
    "        \n",
    "        os.makedirs(download_path, exist_ok=True)\n",
    "        \n",
    "        mlflow.artifacts.download_artifacts(\n",
    "            artifact_uri=model_version.source,\n",
    "            dst_path=download_path\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Model downloaded to: {download_path}\")\n",
    "        \n",
    "        for root, dirs, files in os.walk(download_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"ðŸ“„ {file_path}\")\n",
    "        \n",
    "        return download_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Download error: {e}\")\n",
    "        return None\n",
    "\n",
    "download_path = download_registered_model(\"YOLO_Model_v2\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d06c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo encontrado: /tmp/tmpku0s_fi0/model/best.pt\n",
      "ðŸŽ¯ Modelo YOLO carregado com sucesso!\n",
      "ðŸš€ Modelo pronto para uso!\n"
     ]
    }
   ],
   "source": [
    "def load_yolo_model_from_mlflow(model_name, version=\"1\"):\n",
    "    \n",
    "    try:\n",
    "        model_uri = f\"models:/{model_name}/{version}\"\n",
    "        \n",
    "        client = MlflowClient()\n",
    "        model_version = client.get_model_version(model_name, version)\n",
    "        \n",
    "        import tempfile\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            \n",
    "            mlflow.artifacts.download_artifacts(\n",
    "                artifact_uri=model_version.source,\n",
    "                dst_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            import glob\n",
    "            pt_files = glob.glob(f\"{temp_dir}/**/*.pt\", recursive=True)\n",
    "            \n",
    "            if pt_files:\n",
    "                model_path = pt_files[0]\n",
    "                print(f\"âœ… Model found: {model_path}\")\n",
    "                \n",
    "                try:\n",
    "                    from ultralytics import YOLO\n",
    "                    model = YOLO(model_path)\n",
    "                    print(f\"ðŸŽ¯ YOLO model loaded successfully!\")\n",
    "                    return model\n",
    "                except ImportError:\n",
    "                    print(\"âš ï¸ ultralytics not installed. Returning file path.\")\n",
    "                    return model_path\n",
    "                    \n",
    "            else:\n",
    "                print(\"âŒ .pt file not found in artifacts\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "loaded_model = load_yolo_model_from_mlflow(\"YOLO_Model_v2\", \"1\")\n",
    "\n",
    "if loaded_model:\n",
    "    print(\"ðŸš€ Model ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Gerenciando estÃ¡gios do modelo: YOLO_Model_v2 v1\n",
      "âœ… Modelo promovido para STAGING\n",
      "âœ… DescriÃ§Ã£o atualizada\n",
      "âœ… Tags adicionadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13065/1166312241.py:11: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "def manage_model_stages(model_name, version=\"1\"):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    print(f\"ðŸ”„ Managing model stages: {model_name} v{version}\")\n",
    "    \n",
    "    try:\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=\"Staging\",\n",
    "            archive_existing_versions=False\n",
    "        )\n",
    "        print(\"âœ… Model promoted to STAGING\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error promoting to staging: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            description=\"Model under test - Performance validated on validation dataset\"\n",
    "        )\n",
    "        print(\"âœ… Description updated\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error updating description: {e}\")\n",
    "    \n",
    "    try:\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"performance_tier\",\n",
    "            value=\"high\"\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Tags added\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error adding tags: {e}\")\n",
    "\n",
    "manage_model_stages(\"YOLO_Model_v2\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bf78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Usando modelo existente: YOLO_Model_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13065/4065982064.py:34: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  optimization_date = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
      "2025/11/11 01:14:07 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: YOLO_Model_v2, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Nova versÃ£o registrada!\n",
      "ï¿½ï¿½ Modelo: YOLO_Model_v2\n",
      "ðŸ”¢ VersÃ£o: 2\n",
      "ðŸ”„ date: #2025-11-11_04-14-07\n",
      "ðŸ“Š mAP: 0.87\n",
      "ðŸƒ View run optimization_date_2025-11-11_04-14-07 at: http://localhost:5000/#/experiments/0/runs/632556e7835648b79c15a1771b2c998e\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "def register_optimization_iteration(\n",
    "    model_path, \n",
    "    base_model_name,\n",
    "    optuna_study_name,\n",
    "    best_params,\n",
    "    metrics,\n",
    "    description=\"\"\n",
    "):\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    try:\n",
    "        model = client.get_registered_model(base_model_name)\n",
    "        print(f\"ðŸ“¦ Using existing model: {base_model_name}\")\n",
    "    except:\n",
    "        client.create_registered_model(\n",
    "            name=base_model_name,\n",
    "            description=\"YOLO model with iterative optimizations via Optuna\"\n",
    "        )\n",
    "        print(f\"ðŸ†• Base model created: {base_model_name}\")\n",
    "    \n",
    "    optimization_date = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"optimization_date_{optimization_date}\") as run:\n",
    "        \n",
    "        mlflow.log_param(\"optimization_date\", optimization_date)\n",
    "        mlflow.log_param(\"optuna_study\", optuna_study_name)\n",
    "        mlflow.log_param(\"model_type\", \"YOLO\")\n",
    "        mlflow.log_param(\"optimization_method\", \"Optuna\")\n",
    "        \n",
    "        for param_name, param_value in best_params.items():\n",
    "            mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "        mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
    "        \n",
    "        model_version = client.create_model_version(\n",
    "            name=base_model_name,\n",
    "            source=f\"runs:/{run.info.run_id}/model\",\n",
    "            run_id=run.info.run_id,\n",
    "            description=description or f\"Optimization #{optimization_date} - mAP: {metrics.get('mAP_50', 'N/A')}\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=base_model_name,\n",
    "            version=model_version.version,\n",
    "            key=\"optimization_date\",\n",
    "            value=str(optimization_date)\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=base_model_name,\n",
    "            version=model_version.version,\n",
    "            key=\"optuna_study\",\n",
    "            value=optuna_study_name\n",
    "        )\n",
    "        \n",
    "        print(f\"ï¿½ï¿½ New version registered!\")\n",
    "        print(f\"ðŸ“¦ Model: {base_model_name}\")\n",
    "        print(f\"ðŸ”¢ Version: {model_version.version}\")\n",
    "        print(f\"ðŸ”„ Date: #{optimization_date}\")\n",
    "        print(f\"ðŸ“Š mAP: {metrics.get('mAP_50', 'N/A')}\")\n",
    "        \n",
    "        return model_version\n",
    "\n",
    "def register_next_optimization():\n",
    "    \n",
    "    optimization_data = {\n",
    "        \"model_path\": \"./runs/detect/best_model/weights/last.pt\",\n",
    "        \"base_model_name\": \"YOLO_Model_v2\",\n",
    "        \"optuna_study_name\": \"optuna_study_name\",\n",
    "        \"best_params\": {\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 100,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"weight_decay\": 0.0005\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"mAP_50\": 0.87,\n",
    "            \"mAP_50_95\": 0.74,\n",
    "            \"precision\": 0.84,\n",
    "            \"recall\": 0.81,\n",
    "            \"f1_score\": 0.825\n",
    "        },\n",
    "        \"description\": \"Optimization #2 - Focus on reducing false positives\"\n",
    "    }\n",
    "    \n",
    "    return register_optimization_iteration(**optimization_data)\n",
    "\n",
    "next_version = register_next_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47210d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13065/3826863510.py:9: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  staging_versions = client.get_latest_versions(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Carregando modelo em Staging:\n",
      "ðŸ“¦ Modelo: YOLO_Model_v2 v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Arquivo do modelo: /tmp/tmpu90szjm0/model/best.pt\n",
      "ðŸŽ¯ Modelo YOLO carregado para inferÃªncia!\n",
      "âœ… Modelo v1 pronto para uso!\n"
     ]
    }
   ],
   "source": [
    "def load_staging_model_for_inference(model_name):\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    \n",
    "    staging_versions = client.get_latest_versions(\n",
    "        name=model_name,\n",
    "        stages=[\"Staging\"]\n",
    "    )\n",
    "    \n",
    "    if not staging_versions:\n",
    "        print(f\"âŒ No Staging version for: {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    staging_version = staging_versions[0]\n",
    "    \n",
    "    print(f\"ðŸš€ Loading Staging model:\")\n",
    "    print(f\"ðŸ“¦ Model: {staging_version.name} v{staging_version.version}\")\n",
    "    \n",
    "    try:\n",
    "        model_uri = f\"models:/{model_name}/Staging\"\n",
    "        \n",
    "        import tempfile\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            \n",
    "            mlflow.artifacts.download_artifacts(\n",
    "                artifact_uri=staging_version.source,\n",
    "                dst_path=temp_dir\n",
    "            )\n",
    "            \n",
    "            import glob\n",
    "            pt_files = glob.glob(f\"{temp_dir}/**/*.pt\", recursive=True)\n",
    "            \n",
    "            if pt_files:\n",
    "                model_path = pt_files[0]\n",
    "                print(f\"âœ… Model file: {model_path}\")\n",
    "                \n",
    "                try:\n",
    "                    from ultralytics import YOLO\n",
    "                    model = YOLO(model_path)\n",
    "                    print(f\"ðŸŽ¯ YOLO model loaded for inference!\")\n",
    "                    return model, staging_version\n",
    "                except ImportError:\n",
    "                    print(\"âš ï¸ ultralytics not available. Returning file path.\")\n",
    "                    return model_path, staging_version\n",
    "            else:\n",
    "                print(\"âŒ .pt file not found\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "result = load_staging_model_for_inference(\"YOLO_Model_v2\")\n",
    "if result:\n",
    "    model, staging_version = result\n",
    "    print(f\"âœ… Model v{staging_version.version} ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
