{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3495410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def load_class_names(data_yaml_path):\n",
    "    \"\"\"Load class names from data.yaml file\"\"\"\n",
    "    with open(data_yaml_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data['names']\n",
    "\n",
    "def parse_yolo_annotation(annotation_path):\n",
    "    \"\"\"Parse YOLO annotations from a file\"\"\"\n",
    "    annotations = []\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                class_id = int(parts[0])\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "                annotations.append({\n",
    "                    'class_id': class_id,\n",
    "                    'x_center': x_center,\n",
    "                    'y_center': y_center,\n",
    "                    'width': width,\n",
    "                    'height': height\n",
    "                })\n",
    "    return annotations\n",
    "\n",
    "def get_dataset_info(images_dir, labels_dir, class_names):\n",
    "    \"\"\"Collect information about the entire dataset\"\"\"\n",
    "    dataset_info = []\n",
    "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        label_file = os.path.splitext(image_file)[0] + '.txt'\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            annotations = parse_yolo_annotation(label_path)\n",
    "            \n",
    "            for ann in annotations:\n",
    "                dataset_info.append({\n",
    "                    'image_name': image_file,\n",
    "                    'label_name': label_file,\n",
    "                    'class_id': ann['class_id'],\n",
    "                    'class_name': class_names[ann['class_id']],\n",
    "                    'x_center': ann['x_center'],\n",
    "                    'y_center': ann['y_center'],\n",
    "                    'width': ann['width'],\n",
    "                    'height': ann['height']\n",
    "                })\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "def get_stratified_split_simple(dataset_info, train_ratio=0.8):\n",
    "    \"\"\"Simple stratification - each image goes to the split that needs its classes most\"\"\"\n",
    "    image_classes = defaultdict(set)\n",
    "    for info in dataset_info:\n",
    "        image_classes[info['image_name']].add(info['class_id'])\n",
    "    \n",
    "    images_with_primary_class = []\n",
    "    for image_name, classes in image_classes.items():\n",
    "        primary_class = min(classes)\n",
    "        images_with_primary_class.append((image_name, primary_class))\n",
    "    \n",
    "    images_by_class = defaultdict(list)\n",
    "    for image_name, primary_class in images_with_primary_class:\n",
    "        images_by_class[primary_class].append(image_name)\n",
    "    \n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    \n",
    "    for class_id, images in images_by_class.items():\n",
    "        if len(images) == 1:\n",
    "            train_images.extend(images)\n",
    "        else:\n",
    "            train_imgs, test_imgs = train_test_split(\n",
    "                images, \n",
    "                train_size=train_ratio, \n",
    "                random_state=42,\n",
    "                stratify=None\n",
    "            )\n",
    "            train_images.extend(train_imgs)\n",
    "            test_images.extend(test_imgs)\n",
    "    \n",
    "    return train_images, test_images\n",
    "\n",
    "def create_directory_structure(output_dir, include_drift_accumulative=True):\n",
    "    \"\"\"Create necessary directory structure\"\"\"\n",
    "    dirs_to_create = [\n",
    "        os.path.join(output_dir, 'train', 'images'),\n",
    "        os.path.join(output_dir, 'train', 'labels'),\n",
    "        os.path.join(output_dir, 'test_freeze', 'images'),\n",
    "        os.path.join(output_dir, 'test_freeze', 'labels')\n",
    "    ]\n",
    "    \n",
    "    if include_drift_accumulative:\n",
    "        dirs_to_create.extend([\n",
    "            os.path.join(output_dir, 'test_accumulative', 'images'),\n",
    "            os.path.join(output_dir, 'test_accumulative', 'labels'),\n",
    "            os.path.join(output_dir, 'test_drift', 'images'),\n",
    "            os.path.join(output_dir, 'test_drift', 'labels')\n",
    "        ])\n",
    "    \n",
    "    for dir_path in dirs_to_create:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "def copy_files(image_list, source_images_dir, source_labels_dir, dest_images_dir, dest_labels_dir):\n",
    "    \"\"\"Copy image and label files to destination directories\"\"\"\n",
    "    for image_name in image_list:\n",
    "        source_image_path = os.path.join(source_images_dir, image_name)\n",
    "        dest_image_path = os.path.join(dest_images_dir, image_name)\n",
    "        if os.path.exists(source_image_path):\n",
    "            shutil.copy2(source_image_path, dest_image_path)\n",
    "        \n",
    "        label_name = os.path.splitext(image_name)[0] + '.txt'\n",
    "        source_label_path = os.path.join(source_labels_dir, label_name)\n",
    "        dest_label_path = os.path.join(dest_labels_dir, label_name)\n",
    "        \n",
    "        if os.path.exists(source_label_path):\n",
    "            shutil.copy2(source_label_path, dest_label_path)\n",
    "\n",
    "def load_existing_csv(csv_path):\n",
    "    \"\"\"Load existing CSV if available\"\"\"\n",
    "    if os.path.exists(csv_path):\n",
    "        return pd.read_csv(csv_path)\n",
    "    return None\n",
    "\n",
    "def identify_new_samples(current_dataset_info, existing_df):\n",
    "    \"\"\"Identify new samples that are not in the existing CSV\"\"\"\n",
    "    if existing_df is None:\n",
    "        return current_dataset_info\n",
    "    \n",
    "    existing_images = set(existing_df['image_name'].unique())\n",
    "    current_images = set([info['image_name'] for info in current_dataset_info])\n",
    "    \n",
    "    new_images = current_images - existing_images\n",
    "    \n",
    "    new_dataset_info = [info for info in current_dataset_info if info['image_name'] in new_images]\n",
    "    \n",
    "    return new_dataset_info\n",
    "\n",
    "def manage_test_drift_corrected(output_dir, new_test_images, source_images_dir, source_labels_dir, n_drift):\n",
    "    \"\"\"Manage test_drift as FIFO with n_drift most recent samples\"\"\"\n",
    "    drift_images_dir = os.path.join(output_dir, 'test_drift', 'images')\n",
    "    drift_labels_dir = os.path.join(output_dir, 'test_drift', 'labels')\n",
    "    \n",
    "    existing_drift_images = []\n",
    "    if os.path.exists(drift_images_dir):\n",
    "        existing_drift_images = [f for f in os.listdir(drift_images_dir) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    print(f\"Current test_drift: {len(existing_drift_images)} images\")\n",
    "    print(f\"New images to add: {len(new_test_images)}\")\n",
    "    print(f\"n_drift limit: {n_drift}\")\n",
    "    \n",
    "    total_after_addition = len(existing_drift_images) + len(new_test_images)\n",
    "    \n",
    "    if total_after_addition > n_drift:\n",
    "        num_to_remove = total_after_addition - n_drift\n",
    "        \n",
    "        if num_to_remove >= len(existing_drift_images):\n",
    "            for img in existing_drift_images:\n",
    "                img_path = os.path.join(drift_images_dir, img)\n",
    "                if os.path.exists(img_path):\n",
    "                    os.remove(img_path)\n",
    "                \n",
    "                label_name = os.path.splitext(img)[0] + '.txt'\n",
    "                label_path = os.path.join(drift_labels_dir, label_name)\n",
    "                if os.path.exists(label_path):\n",
    "                    os.remove(label_path)\n",
    "            \n",
    "            if len(new_test_images) > n_drift:\n",
    "                images_to_add = new_test_images[-n_drift:]\n",
    "                print(f\"Too many new images. Adding only the last {n_drift}\")\n",
    "            else:\n",
    "                images_to_add = new_test_images\n",
    "        else:\n",
    "            images_to_remove = existing_drift_images[:num_to_remove]\n",
    "            print(f\"Removing {len(images_to_remove)} old images\")\n",
    "            \n",
    "            for img in images_to_remove:\n",
    "                img_path = os.path.join(drift_images_dir, img)\n",
    "                if os.path.exists(img_path):\n",
    "                    os.remove(img_path)\n",
    "                \n",
    "                label_name = os.path.splitext(img)[0] + '.txt'\n",
    "                label_path = os.path.join(drift_labels_dir, label_name)\n",
    "                if os.path.exists(label_path):\n",
    "                    os.remove(label_path)\n",
    "            \n",
    "            images_to_add = new_test_images\n",
    "    else:\n",
    "        images_to_add = new_test_images\n",
    "    \n",
    "    print(f\"Adding {len(images_to_add)} images to test_drift\")\n",
    "    copy_files(images_to_add, source_images_dir, source_labels_dir, \n",
    "               drift_images_dir, drift_labels_dir)\n",
    "    \n",
    "    final_drift_images = []\n",
    "    if os.path.exists(drift_images_dir):\n",
    "        final_drift_images = [f for f in os.listdir(drift_images_dir) \n",
    "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    print(f\"Final test_drift: {len(final_drift_images)} images\")\n",
    "    \n",
    "    return final_drift_images\n",
    "\n",
    "def sync_csv_with_physical_directories(csv_path, output_dir):\n",
    "    \"\"\"Synchronize CSV with physical directories to ensure consistency\"\"\"\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    split_dirs = {\n",
    "        'train': os.path.join(output_dir, 'train', 'images'),\n",
    "        'test_freeze': os.path.join(output_dir, 'test_freeze', 'images'),\n",
    "        'test_accumulative': os.path.join(output_dir, 'test_accumulative', 'images'),\n",
    "        'test_drift': os.path.join(output_dir, 'test_drift', 'images')\n",
    "    }\n",
    "    \n",
    "    physical_images = {}\n",
    "    for split_name, split_dir in split_dirs.items():\n",
    "        if os.path.exists(split_dir):\n",
    "            images = set([f for f in os.listdir(split_dir) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            physical_images[split_name] = images\n",
    "        else:\n",
    "            physical_images[split_name] = set()\n",
    "    \n",
    "    print(f\"Synchronizing CSV with physical directories...\")\n",
    "    print(f\"  - train: {len(physical_images['train'])} images\")\n",
    "    print(f\"  - test_freeze: {len(physical_images['test_freeze'])} images\")\n",
    "    print(f\"  - test_accumulative: {len(physical_images['test_accumulative'])} images\")\n",
    "    print(f\"  - test_drift: {len(physical_images['test_drift'])} images\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        current_splits = []\n",
    "        \n",
    "        for split_name, images in physical_images.items():\n",
    "            if image_name in images:\n",
    "                current_splits.append(split_name)\n",
    "        \n",
    "        old_splits = [s.strip() for s in str(row['splits']).split(',') if s.strip()]\n",
    "        if 'test_new' in old_splits:\n",
    "            test_splits = [s for s in current_splits if s in ['test_freeze', 'test_accumulative', 'test_drift']]\n",
    "            if not test_splits:\n",
    "                current_splits.append('test_new')\n",
    "        \n",
    "        df.at[idx, 'splits'] = ','.join(current_splits) if current_splits else 'unknown'\n",
    "        df.at[idx, 'split'] = current_splits[0] if current_splits else 'unknown'\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"CSV synchronized with physical directories!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_csv_with_splits_corrected(dataset_info, existing_df, train_images, test_images, \n",
    "                                    test_accumulative_images, test_drift_images, output_dir, \n",
    "                                    add_to_test_freeze, iteration):\n",
    "    \"\"\"Update or create CSV with information from all splits\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    for info in dataset_info:\n",
    "        info['iteration'] = iteration\n",
    "        info['timestamp'] = timestamp\n",
    "        \n",
    "        splits = []\n",
    "        \n",
    "        if info['image_name'] in train_images:\n",
    "            splits.append('train')\n",
    "        \n",
    "        if info['image_name'] in test_images:\n",
    "            if add_to_test_freeze:\n",
    "                splits.append('test_freeze')\n",
    "            splits.append('test_new')\n",
    "        \n",
    "        if info['image_name'] in test_accumulative_images:\n",
    "            splits.append('test_accumulative')\n",
    "        \n",
    "        if info['image_name'] in test_drift_images:\n",
    "            splits.append('test_drift')\n",
    "        \n",
    "        info['splits'] = ','.join(splits) if splits else 'unknown'\n",
    "        info['split'] = splits[0] if splits else 'unknown'\n",
    "    \n",
    "    new_df = pd.DataFrame(dataset_info)\n",
    "    \n",
    "    if existing_df is not None:\n",
    "        existing_df['splits'] = existing_df['splits'].apply(\n",
    "            lambda x: ','.join([s for s in str(x).split(',') if s.strip() != 'test_drift']) \n",
    "            if pd.notna(x) else ''\n",
    "        )\n",
    "        \n",
    "        existing_df['splits'] = existing_df['splits'].apply(\n",
    "            lambda x: x if x and x != '' else 'unknown'\n",
    "        )\n",
    "        \n",
    "        for idx, row in existing_df.iterrows():\n",
    "            if row['image_name'] in test_drift_images:\n",
    "                current_splits = [s.strip() for s in str(row['splits']).split(',') if s.strip() and s.strip() != 'unknown']\n",
    "                if 'test_drift' not in current_splits:\n",
    "                    current_splits.append('test_drift')\n",
    "                existing_df.at[idx, 'splits'] = ','.join(current_splits)\n",
    "        \n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "    \n",
    "    csv_path = os.path.join(output_dir, 'dataset_info.csv')\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def print_statistics(df, class_names, iteration=1):\n",
    "    \"\"\"Print dataset statistics\"\"\"\n",
    "    print(f\"\\n=== DATASET STATISTICS (Iteration {iteration}) ===\")\n",
    "    \n",
    "    total_images = df['image_name'].nunique()\n",
    "    total_annotations = len(df)\n",
    "    \n",
    "    print(f\"Total images: {total_images}\")\n",
    "    print(f\"Total annotations: {total_annotations}\")\n",
    "    \n",
    "    current_iteration_df = df[df['iteration'] == iteration]\n",
    "    if not current_iteration_df.empty:\n",
    "        current_images = current_iteration_df['image_name'].nunique()\n",
    "        current_annotations = len(current_iteration_df)\n",
    "        print(f\"New images (iteration {iteration}): {current_images}\")\n",
    "        print(f\"New annotations (iteration {iteration}): {current_annotations}\")\n",
    "    \n",
    "    print(\"\\n--- Distribution by Split ---\")\n",
    "    if 'splits' in df.columns:\n",
    "        all_splits = set()\n",
    "        for splits_str in df['splits'].dropna():\n",
    "            if str(splits_str) != 'unknown':\n",
    "                all_splits.update(str(splits_str).split(','))\n",
    "        \n",
    "        for split_type in sorted(all_splits):\n",
    "            split_type = split_type.strip()\n",
    "            if split_type and split_type != 'unknown':\n",
    "                count = df[df['splits'].str.contains(split_type, na=False)]['image_name'].nunique()\n",
    "                percentage = (count / total_images) * 100\n",
    "                print(f\"{split_type}: {count} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n--- Distribution by Class ---\")\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = (count / total_annotations) * 100\n",
    "        print(f\"{class_name}: {count} annotations ({percentage:.1f}%)\")\n",
    "\n",
    "def split_yolo_dataset(central_dir, output_dir, train_ratio=0.8, data_yaml_path=None,\n",
    "                      csv_info=None, add_to_test_freeze=False, n_drift=100):\n",
    "    \"\"\"\n",
    "    Main function to split YOLO dataset in a stratified and incremental way\n",
    "    \n",
    "    Args:\n",
    "        central_dir: Directory containing images/ and labels/ subfolders\n",
    "        output_dir: Output directory where train/ and test_freeze/ folders will be created\n",
    "        train_ratio: Proportion of data for training (default: 0.8)\n",
    "        data_yaml_path: Path to data.yaml file (optional)\n",
    "        csv_info: Path to CSV with previous information (optional)\n",
    "        add_to_test_freeze: Whether to add new test to test_freeze (default: False)\n",
    "        n_drift: Maximum number of samples in test_drift (default: 100)\n",
    "    \"\"\"\n",
    "    \n",
    "    images_dir = os.path.join(central_dir, 'images')\n",
    "    labels_dir = os.path.join(central_dir, 'labels')\n",
    "    \n",
    "    if csv_info is None:\n",
    "        csv_info = os.path.join(output_dir, 'dataset_info.csv')\n",
    "    \n",
    "    existing_df = load_existing_csv(csv_info)\n",
    "    is_first_run = existing_df is None\n",
    "    \n",
    "    iteration = 1 if is_first_run else existing_df['iteration'].max() + 1\n",
    "    \n",
    "    print(f\"=== {'FIRST EXECUTION' if is_first_run else f'INCREMENTAL EXECUTION (Iteration {iteration})'} ===\")\n",
    "    \n",
    "    if data_yaml_path is None:\n",
    "        data_yaml_path = os.path.join(central_dir, 'data.yaml')\n",
    "    \n",
    "    if os.path.exists(data_yaml_path):\n",
    "        class_names = load_class_names(data_yaml_path)\n",
    "    else:\n",
    "        print(\"Warning: data.yaml not found. Using generic class names.\")\n",
    "        max_class_id = 0\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            if label_file.endswith('.txt'):\n",
    "                with open(os.path.join(labels_dir, label_file), 'r') as f:\n",
    "                    for line in f:\n",
    "                        if line.strip():\n",
    "                            class_id = int(line.split()[0])\n",
    "                            max_class_id = max(max_class_id, class_id)\n",
    "        class_names = [f'class_{i}' for i in range(max_class_id + 1)]\n",
    "    \n",
    "    print(f\"Classes found: {class_names}\")\n",
    "    \n",
    "    print(\"Collecting dataset information...\")\n",
    "    current_dataset_info = get_dataset_info(images_dir, labels_dir, class_names)\n",
    "    \n",
    "    new_dataset_info = identify_new_samples(current_dataset_info, existing_df)\n",
    "    \n",
    "    if not new_dataset_info:\n",
    "        print(\"No new samples found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(set([info['image_name'] for info in new_dataset_info]))} new images\")\n",
    "    \n",
    "    print(\"Performing stratified split of new samples...\")\n",
    "    train_images, test_images = get_stratified_split_simple(new_dataset_info, train_ratio)\n",
    "    \n",
    "    print(f\"New samples - Train: {len(train_images)}, Test: {len(test_images)}\")\n",
    "    \n",
    "    print(\"Creating/checking directory structure...\")\n",
    "    create_directory_structure(output_dir, include_drift_accumulative=True)\n",
    "    \n",
    "    print(\"Copying new files to train/...\")\n",
    "    copy_files(\n",
    "        train_images, \n",
    "        images_dir, \n",
    "        labels_dir,\n",
    "        os.path.join(output_dir, 'train', 'images'),\n",
    "        os.path.join(output_dir, 'train', 'labels')\n",
    "    )\n",
    "    \n",
    "    if add_to_test_freeze:\n",
    "        print(\"Adding new tests to test_freeze/...\")\n",
    "        copy_files(\n",
    "            test_images,\n",
    "            images_dir,\n",
    "            labels_dir, \n",
    "            os.path.join(output_dir, 'test_freeze', 'images'),\n",
    "            os.path.join(output_dir, 'test_freeze', 'labels')\n",
    "        )\n",
    "    \n",
    "    if is_first_run:\n",
    "        print(\"Copying files to test_freeze/ (first execution)...\")\n",
    "        copy_files(\n",
    "            test_images,\n",
    "            images_dir,\n",
    "            labels_dir, \n",
    "            os.path.join(output_dir, 'test_freeze', 'images'),\n",
    "            os.path.join(output_dir, 'test_freeze', 'labels')\n",
    "        )\n",
    "    \n",
    "    print(\"Adding to test_accumulative...\")\n",
    "    copy_files(\n",
    "        test_images,\n",
    "        images_dir,\n",
    "        labels_dir,\n",
    "        os.path.join(output_dir, 'test_accumulative', 'images'),\n",
    "        os.path.join(output_dir, 'test_accumulative', 'labels')\n",
    "    )\n",
    "    \n",
    "    print(\"Managing test_drift...\")\n",
    "    final_drift_images = manage_test_drift_corrected(\n",
    "        output_dir, test_images, images_dir, labels_dir, n_drift\n",
    "    )\n",
    "    \n",
    "    if is_first_run:\n",
    "        print(\"Configuring test_accumulative and test_drift (first execution)...\")\n",
    "        test_accumulative_images = test_images.copy()\n",
    "        test_drift_images = final_drift_images\n",
    "    else:\n",
    "        test_accumulative_images = test_images.copy()\n",
    "        test_drift_images = final_drift_images\n",
    "    \n",
    "    print(\"Updating CSV report...\")\n",
    "    df = update_csv_with_splits_corrected(\n",
    "        new_dataset_info, existing_df, train_images, test_images,\n",
    "        test_accumulative_images, test_drift_images, output_dir,\n",
    "        add_to_test_freeze, iteration\n",
    "    )\n",
    "    \n",
    "    print(\"Synchronizing CSV with physical directories...\")\n",
    "    df = sync_csv_with_physical_directories(\n",
    "        os.path.join(output_dir, 'dataset_info.csv'), \n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    print_statistics(df, class_names, iteration)\n",
    "    \n",
    "    print(f\"\\nSplit completed! Files saved in: {output_dir}\")\n",
    "    print(f\"CSV report updated in: {os.path.join(output_dir, 'dataset_info.csv')}\")\n",
    "    print(f\"Configuration used:\")\n",
    "    print(f\"  - add_to_test_freeze: {add_to_test_freeze}\")\n",
    "    print(f\"  - n_drift: {n_drift}\")\n",
    "    print(f\"  - train_ratio: {train_ratio}\")\n",
    "\n",
    "def recreate_splits_from_csv(csv_path, source_data_dir, output_dir, iteration=None):\n",
    "    \"\"\"\n",
    "    Recreate all splits from metadata CSV\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to dataset_info.csv\n",
    "        source_data_dir: Directory with original data (images/ and labels/)\n",
    "        output_dir: Directory where to recreate splits\n",
    "        iteration: Specific iteration to recreate (None = all)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if iteration is not None:\n",
    "        df = df[df['iteration'] == iteration]\n",
    "        print(f\"Recreating splits only from iteration {iteration}\")\n",
    "    else:\n",
    "        print(\"Recreating splits from all iterations\")\n",
    "    \n",
    "    create_directory_structure(output_dir, include_drift_accumulative=True)\n",
    "    \n",
    "    source_images_dir = os.path.join(source_data_dir, 'images')\n",
    "    source_labels_dir = os.path.join(source_data_dir, 'labels')\n",
    "    \n",
    "    splits_dict = {\n",
    "        'train': set(),\n",
    "        'test_freeze': set(),\n",
    "        'test_accumulative': set(),\n",
    "        'test_drift': set(),\n",
    "        'test_new': set()\n",
    "    }\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        splits = str(row['splits']).split(',') if pd.notna(row['splits']) else []\n",
    "        \n",
    "        for split in splits:\n",
    "            split = split.strip()\n",
    "            if split in splits_dict:\n",
    "                splits_dict[split].add(image_name)\n",
    "    \n",
    "    for split_name, images in splits_dict.items():\n",
    "        if images:\n",
    "            dest_images_dir = os.path.join(output_dir, split_name, 'images')\n",
    "            dest_labels_dir = os.path.join(output_dir, split_name, 'labels')\n",
    "            \n",
    "            print(f\"Recreating {split_name}/: {len(images)} images\")\n",
    "            copy_files(list(images), source_images_dir, source_labels_dir, \n",
    "                      dest_images_dir, dest_labels_dir)\n",
    "    \n",
    "    print(f\"\\nSplits recreated successfully in: {output_dir}\")\n",
    "    \n",
    "    print(\"\\n=== RECREATED SPLITS STATISTICS ===\")\n",
    "    for split_name, images in splits_dict.items():\n",
    "        if images:\n",
    "            print(f\"{split_name}: {len(images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097c7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST EXECUTION ===\n",
      "Classes found: ['Paper', 'Rock', 'Scissors']\n",
      "Collecting dataset information...\n",
      "Found 858 new images\n",
      "Performing stratified split of new samples...\n",
      "New samples - Train: 685, Test: 173\n",
      "Creating/checking directory structure...\n",
      "Copying new files to train/...\n",
      "Copying files to test_freeze/ (first execution)...\n",
      "Adding to test_accumulative...\n",
      "Managing test_drift...\n",
      "Current test_drift: 0 images\n",
      "New images to add: 173\n",
      "n_drift limit: 50\n",
      "Too many new images. Adding only the last 50\n",
      "Adding 50 images to test_drift\n",
      "Final test_drift: 50 images\n",
      "Configuring test_accumulative and test_drift (first execution)...\n",
      "Updating CSV report...\n",
      "Synchronizing CSV with physical directories...\n",
      "Synchronizing CSV with physical directories...\n",
      "  - train: 685 images\n",
      "  - test_freeze: 173 images\n",
      "  - test_accumulative: 173 images\n",
      "  - test_drift: 50 images\n",
      "CSV synchronized with physical directories!\n",
      "\n",
      "=== DATASET STATISTICS (Iteration 1) ===\n",
      "Total images: 858\n",
      "Total annotations: 999\n",
      "New images (iteration 1): 858\n",
      "New annotations (iteration 1): 999\n",
      "\n",
      "--- Distribution by Split ---\n",
      "test_accumulative: 173 images (20.2%)\n",
      "test_drift: 50 images (5.8%)\n",
      "test_freeze: 173 images (20.2%)\n",
      "train: 685 images (79.8%)\n",
      "\n",
      "--- Distribution by Class ---\n",
      "Rock: 416 annotations (41.6%)\n",
      "Paper: 293 annotations (29.3%)\n",
      "Scissors: 290 annotations (29.0%)\n",
      "\n",
      "Split completed! Files saved in: ./dataset\n",
      "CSV report updated in: ./dataset/dataset_info.csv\n",
      "Configuration used:\n",
      "  - add_to_test_freeze: False\n",
      "  - n_drift: 50\n",
      "  - train_ratio: 0.8\n"
     ]
    }
   ],
   "source": [
    "split_yolo_dataset(\n",
    "    central_dir=\"./data\",\n",
    "    output_dir=\"./dataset\",\n",
    "    train_ratio=0.8,\n",
    "    csv_info=None,\n",
    "    add_to_test_freeze=False,\n",
    "    n_drift=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b1e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INCREMENTAL EXECUTION (Iteration 2) ===\n",
      "Classes found: ['Paper', 'Rock', 'Scissors']\n",
      "Collecting dataset information...\n",
      "Found 908 new images\n",
      "Performing stratified split of new samples...\n",
      "New samples - Train: 725, Test: 183\n",
      "Creating/checking directory structure...\n",
      "Copying new files to train/...\n",
      "Adding to test_accumulative...\n",
      "Managing test_drift...\n",
      "Current test_drift: 50 images\n",
      "New images to add: 183\n",
      "n_drift limit: 50\n",
      "Too many new images. Adding only the last 50\n",
      "Adding 50 images to test_drift\n",
      "Final test_drift: 50 images\n",
      "Updating CSV report...\n",
      "Synchronizing CSV with physical directories...\n",
      "Synchronizing CSV with physical directories...\n",
      "  - train: 1410 images\n",
      "  - test_freeze: 173 images\n",
      "  - test_accumulative: 356 images\n",
      "  - test_drift: 50 images\n",
      "CSV synchronized with physical directories!\n",
      "\n",
      "=== DATASET STATISTICS (Iteration 2) ===\n",
      "Total images: 1766\n",
      "Total annotations: 2062\n",
      "New images (iteration 2): 908\n",
      "New annotations (iteration 2): 1063\n",
      "\n",
      "--- Distribution by Split ---\n",
      "test_accumulative: 356 images (20.2%)\n",
      "test_drift: 50 images (2.8%)\n",
      "test_freeze: 173 images (9.8%)\n",
      "train: 1410 images (79.8%)\n",
      "\n",
      "--- Distribution by Class ---\n",
      "Rock: 854 annotations (41.4%)\n",
      "Paper: 626 annotations (30.4%)\n",
      "Scissors: 582 annotations (28.2%)\n",
      "\n",
      "Split completed! Files saved in: ./dataset\n",
      "CSV report updated in: ./dataset/dataset_info.csv\n",
      "Configuration used:\n",
      "  - add_to_test_freeze: False\n",
      "  - n_drift: 50\n",
      "  - train_ratio: 0.8\n"
     ]
    }
   ],
   "source": [
    "split_yolo_dataset(\n",
    "    central_dir=\"./data\",\n",
    "    output_dir=\"./dataset\",\n",
    "    add_to_test_freeze=False,\n",
    "    n_drift=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4e4b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_splits_from_csv(csv_path, source_data_dir, output_dir, iteration=None):\n",
    "    \"\"\"\n",
    "    Recreate all splits from metadata CSV\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to dataset_info.csv\n",
    "        source_data_dir: Directory with original data (images/ and labels/)\n",
    "        output_dir: Directory where to recreate splits\n",
    "        iteration: Specific iteration to recreate (None = all)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if iteration is not None:\n",
    "        df = df[df['iteration'] == iteration]\n",
    "        print(f\"Recreating splits only from iteration {iteration}\")\n",
    "    else:\n",
    "        print(\"Recreating splits from all iterations\")\n",
    "    \n",
    "    create_directory_structure(output_dir, include_drift_accumulative=True)\n",
    "    \n",
    "    source_images_dir = os.path.join(source_data_dir, 'images')\n",
    "    source_labels_dir = os.path.join(source_data_dir, 'labels')\n",
    "    \n",
    "    splits_dict = {\n",
    "        'train': set(),\n",
    "        'test_freeze': set(),\n",
    "        'test_accumulative': set(),\n",
    "        'test_drift': set(),\n",
    "        'test_new': set()\n",
    "    }\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        splits = row['splits'].split(',') if pd.notna(row['splits']) else []\n",
    "        \n",
    "        for split in splits:\n",
    "            split = split.strip()\n",
    "            if split in splits_dict:\n",
    "                splits_dict[split].add(image_name)\n",
    "    \n",
    "    for split_name, images in splits_dict.items():\n",
    "        if images:\n",
    "            dest_images_dir = os.path.join(output_dir, split_name, 'images')\n",
    "            dest_labels_dir = os.path.join(output_dir, split_name, 'labels')\n",
    "            \n",
    "            print(f\"Recreating {split_name}/: {len(images)} images\")\n",
    "            copy_files(list(images), source_images_dir, source_labels_dir, \n",
    "                      dest_images_dir, dest_labels_dir)\n",
    "    \n",
    "    print(f\"\\nSplits recreated successfully in: {output_dir}\")\n",
    "    \n",
    "    print(\"\\n=== RECREATED SPLITS STATISTICS ===\")\n",
    "    for split_name, images in splits_dict.items():\n",
    "        if images:\n",
    "            print(f\"{split_name}: {len(images)} images\")\n",
    "\n",
    "def recreate_specific_split(csv_path, source_data_dir, output_dir, split_name, iteration=None):\n",
    "    \"\"\"\n",
    "    Recreate only a specific split\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to dataset_info.csv\n",
    "        source_data_dir: Directory with original data\n",
    "        output_dir: Output directory\n",
    "        split_name: Split name ('train', 'test_freeze', etc.)\n",
    "        iteration: Specific iteration (None = all)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if iteration is not None:\n",
    "        df = df[df['iteration'] == iteration]\n",
    "    \n",
    "    split_images = set()\n",
    "    for _, row in df.iterrows():\n",
    "        splits = row['splits'].split(',') if pd.notna(row['splits']) else []\n",
    "        if split_name in [s.strip() for s in splits]:\n",
    "            split_images.add(row['image_name'])\n",
    "    \n",
    "    if not split_images:\n",
    "        print(f\"No images found for split '{split_name}'\")\n",
    "        return\n",
    "    \n",
    "    dest_images_dir = os.path.join(output_dir, split_name, 'images')\n",
    "    dest_labels_dir = os.path.join(output_dir, split_name, 'labels')\n",
    "    os.makedirs(dest_images_dir, exist_ok=True)\n",
    "    os.makedirs(dest_labels_dir, exist_ok=True)\n",
    "    \n",
    "    source_images_dir = os.path.join(source_data_dir, 'images')\n",
    "    source_labels_dir = os.path.join(source_data_dir, 'labels')\n",
    "    \n",
    "    print(f\"Recreating {split_name}/: {len(split_images)} images\")\n",
    "    copy_files(list(split_images), source_images_dir, source_labels_dir,\n",
    "              dest_images_dir, dest_labels_dir)\n",
    "    \n",
    "    print(f\"Split '{split_name}' recreated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ac36cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating splits from all iterations\n",
      "Recreating train/: 1410 images\n",
      "Recreating test_freeze/: 173 images\n",
      "Recreating test_accumulative/: 356 images\n",
      "Recreating test_drift/: 50 images\n",
      "\n",
      "Splits recreated successfully in: dataset_debug\n",
      "\n",
      "=== RECREATED SPLITS STATISTICS ===\n",
      "train: 1410 images\n",
      "test_freeze: 173 images\n",
      "test_accumulative: 356 images\n",
      "test_drift: 50 images\n"
     ]
    }
   ],
   "source": [
    "recreate_splits_from_csv(\n",
    "    csv_path=\"dataset/dataset_info.csv\",\n",
    "    source_data_dir=\"data\",\n",
    "    output_dir=\"dataset_debug\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_splits_consistency(csv_path, splits_dir):\n",
    "    \"\"\"\n",
    "    Validate if physical splits are consistent with CSV\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    csv_counts = {}\n",
    "    splits_dict = {\n",
    "        'train': set(),\n",
    "        'test_freeze': set(),\n",
    "        'test_accumulative': set(),\n",
    "        'test_drift': set()\n",
    "    }\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        splits = row['splits'].split(',') if pd.notna(row['splits']) else []\n",
    "        for split in splits:\n",
    "            split = split.strip()\n",
    "            if split in splits_dict:\n",
    "                splits_dict[split].add(row['image_name'])\n",
    "    \n",
    "    physical_counts = {}\n",
    "    for split_name in splits_dict.keys():\n",
    "        split_images_dir = os.path.join(splits_dir, split_name, 'images')\n",
    "        if os.path.exists(split_images_dir):\n",
    "            images = [f for f in os.listdir(split_images_dir) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            physical_counts[split_name] = len(images)\n",
    "        else:\n",
    "            physical_counts[split_name] = 0\n",
    "    \n",
    "    print(\"=== CONSISTENCY VALIDATION ===\")\n",
    "    all_consistent = True\n",
    "    for split_name in splits_dict.keys():\n",
    "        csv_count = len(splits_dict[split_name])\n",
    "        physical_count = physical_counts[split_name]\n",
    "        \n",
    "        status = \"‚úÖ\" if csv_count == physical_count else \"‚ùå\"\n",
    "        print(f\"{split_name}: CSV={csv_count}, Physical={physical_count} {status}\")\n",
    "        \n",
    "        if csv_count != physical_count:\n",
    "            all_consistent = False\n",
    "    \n",
    "    if all_consistent:\n",
    "        print(\"\\nüéØ All splits are consistent!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Inconsistencies detected. Run recreate_splits_from_csv()\")\n",
    "    \n",
    "    return all_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b25d7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONSISTENCY VALIDATION ===\n",
      "train: CSV=1410, Physical=1410 ‚úÖ\n",
      "test_freeze: CSV=173, Physical=173 ‚úÖ\n",
      "test_accumulative: CSV=356, Physical=356 ‚úÖ\n",
      "test_drift: CSV=50, Physical=50 ‚úÖ\n",
      "\n",
      "üéØ All splits are consistent!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_splits_consistency(csv_path=\"dataset/dataset_info.csv\", splits_dir=\"dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
